<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>library.classes.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>library.classes.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import numpy as np
import tensorflow as tf
from library.classes.losses import BackmappingRelativeVectorLoss
from library.classes.generators import PADDING_X, PADDING_Y

import time
import matplotlib.pyplot as plt


class CNN:
    def __init__(
        self,
        input_size: tuple,
        output_size: tuple,
        display_name: str,
        data_prefix: str,
        keep_checkpoints: bool = False,
        load_path: str = None,
        loss: tf.keras.losses.Loss = tf.keras.losses.MeanSquaredError(),
        test_sample: tuple = None,
    ):
        &#34;&#34;&#34;
        This is the base class for all CNNs. It contains the basic structure of the CNN and the fit function.

        Args:
            input_size (tuple): The size of the input. Should be (x, y, 1)
            output_size (tuple): The size of the output. Should be (x, y, 1)
            display_name (str): The name of the model. Used for displaying the model summary and saving checkpoints/logs.
            data_prefix (str): The prefix for all data. Used for saving the model and saving tensorboard logs.
            keep_checkpoints (bool, optional): If true checkpoints will be kept. Defaults to False.
            load_path (str, optional): The path to the weights to load. Defaults to None.
            loss (tf.keras.losses.Loss, optional): The loss function to use. Defaults to tf.keras.losses.MeanSquaredError().
            test_sample (tuple, optional): The test sample to track after each epoch. Defaults to None.
        &#34;&#34;&#34;
        super().__init__()

        self.display_name = display_name
        self.keep_checkpoints = keep_checkpoints
        self.load_path = load_path
        self.data_prefix = data_prefix
        self.loss = loss
        self.test_sample = test_sample
        self.current_epoch = 0

        # For showing samples
        self.fig = plt.figure()
        self.ax = self.fig.add_subplot(111, projection=&#39;3d&#39;)

        # Scale the model, this currently only affects the number of filters
        scale = 32          # Scaled the filter dimensions by this factor

        # The activation function to use for the convolutional layers
        conv_activation = tf.keras.layers.LeakyReLU(alpha=0.01)

        self.model = tf.keras.Sequential(
            [
                ##### Input layer #####
                tf.keras.layers.Input(input_size, sparse=False),
                ##### Encoder #####
                tf.keras.layers.Conv2D(
                    filters=2**0 * scale,
                    kernel_size=(2, 2),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**1 * scale,
                    kernel_size=(2, 2),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**2 * scale,
                    kernel_size=(2, 1),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**3 * scale,
                    kernel_size=(2, 1),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**4 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**5 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**6 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**7 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.MaxPool2D(
                    pool_size=(3, 3),
                ),
                tf.keras.layers.BatchNormalization(),
                ##### Output #####
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(np.prod(output_size), activation=&#39;tanh&#39;, kernel_initializer=tf.keras.initializers.Zeros()),  # tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)),
                tf.keras.layers.Reshape(output_size)
            ], name=self.display_name)

        # Compile the model
        self.model.compile(
            optimizer=tf.optimizers.Adam(
                # learning_rate=0.00001, # Start with 1% of the default learning rate
            ),
            loss=self.loss,
            metrics=[&#39;accuracy&#39;, &#39;mae&#39;],
            run_eagerly=True
        )

        self.model.summary()

        # Load weights if path is given
        if load_path is not None and os.path.exists(load_path):
            self.model.load_weights(load_path)
            print(&#34;Loaded weights from &#34; + load_path)

    def fit(self,
            train_generator,
            validation_gen,
            epochs=150,
            batch_size=64,
            verbose=1,
            early_stop=False,
            ):
        &#34;&#34;&#34;
            Trains the model with the given data

            Args:
                train_generator (tf.keras.utils.Sequence): The training data generator
                validation_gen (tf.keras.utils.Sequence): The validation data generator
                epochs (int, optional): The number of epochs to train. Defaults to 150.
                batch_size (int, optional): The batch size. Defaults to 64.
                verbose (int, optional): The verbosity level. Defaults to 1.
                early_stop (bool, optional): If true early stop will be used. Defaults to False.
        &#34;&#34;&#34;

        # Create hist dir if it does not exist
        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

        callbacks = [
            # The BackupAndRestore callback saves checkpoints every save_freq batches,
            # this is set to 1 epoch here to save after each epoch. Make sure to set
            # this to a value greater than one when training on a large dataset, because
            # it can take a long time to save checkpoints.
            tf.keras.callbacks.experimental.BackupAndRestore(
                backup_dir=os.path.join(self.data_prefix, &#34;backup&#34;, self.display_name),
                # save_freq=1,
                delete_checkpoint=not self.keep_checkpoints,
                # save_before_preemption=False,
            ),

            # The ReduceLROnPlateau callback monitors a quantity and if no improvement
            # is seen for a &#39;patience&#39; number of epochs, the learning rate is reduced.
            # Here, it monitors &#39;val_loss&#39; and if no improvement is seen for 10 epochs,
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor=&#39;val_loss&#39;,
                factor=0.75,
                patience=3,
                verbose=0,
                mode=&#39;max&#39;,
                min_delta=0.00000005,
                cooldown=5,
                min_lr=0.0000000000001,
            ),

            # The CSVLogger callback streams epoch results to a CSV file.
            # #TODO Send the CSV file via email or telegram after each epoch
            # to monitor the training progress.
            tf.keras.callbacks.CSVLogger(
                os.path.join(self.data_prefix, &#34;hist&#34;, f&#34;training_history_{self.display_name}.csv&#34;),
                separator=&#39;,&#39;,
                append=True,
            ),

            # The TensorBoard callback writes a log for TensorBoard, which allows
            # you to visualize dynamic graphs of your training and test metrics,
            # as well as activation histograms for the different layers in your model.
            tf.keras.callbacks.TensorBoard(
                log_dir=os.path.join(self.data_prefix, &#39;tensorboard&#39;, self.display_name),
                histogram_freq=1,
                write_graph=True,
                write_images=True,
                update_freq=&#39;batch&#39;,
            ),
            # Update the current epoch and in the future other stuff
            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: self.update_internals(epoch, logs)),
            # Track the test sample after each epoch
            # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda epoch, logs: self.track_test_samle(epoch, logs)),
            # Track the test sample after each batch (live)
            # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.live_track_test_samle(batch, logs)),
            # Add custom metrics to tensorboard
            tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.custom_tensorboard_metrics(batch, logs)),
        ]

        if early_stop:
            # The EarlyStopping callback is used to exit the training early if
            # the validation loss does not improve after 20 epochs.
            # This makes sure that the model does not overfit the training data.
            callbacks.append(
                tf.keras.callbacks.EarlyStopping(
                    monitor=&#39;val_loss&#39;,
                    min_delta=0.0001,
                    patience=20,
                    verbose=0,
                    mode=&#39;auto&#39;,
                    baseline=None,
                    restore_best_weights=True,
                ))

        # Open plot for live tracking TODO: fix this
        # self.fig.show()
        # self.fig.canvas.draw()

        # Here we use generators to generate batches of data for the training.
        # The training data is currently generated by the RelativeVectorsTrainingDataGenerator
        # and is the main thing that changes the input data structure to the desired output.
        self.hist = self.model.fit(
            train_generator,
            validation_data=validation_gen,
            batch_size=batch_size,
            shuffle=True,
            epochs=epochs,
            verbose=verbose,
            callbacks=callbacks,
        )

    def track_test_samle(self, batch, logs=None):
        &#34;&#34;&#34;
            Tracks the test sample after each batch and saves the figure to the hist folder.

            Args:
                batch (int): The current batch
                logs (dict, optional): The logs from the training. Defaults to None.
        &#34;&#34;&#34;

        # Predict the output
        pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
        true = self.test_sample[1][0, :, :, 0]

        # Remove padding
        pred = pred[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]
        true = true[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]

        # Make 3D scatter
        pred_positions = []
        true_positions = []
        for i, bond in enumerate(pred):
            pred_positions.append(pred[i, :])
        for i, bond in enumerate(true):
            true_positions.append(true[i, :])

        fig = plt.figure()
        ax = fig.add_subplot(111, projection=&#39;3d&#39;)

        ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
        ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

        # Draw lines between the atoms predicted and true positions
        for i in range(len(true_positions)):
            ax.plot([true_positions[i][0], pred_positions[i][0]], [true_positions[i][1], pred_positions[i][1]], [true_positions[i][2], pred_positions[i][2]], color=&#39;gray&#39;, linestyle=&#39;-&#39;, linewidth=0.1)

        # Add legend
        ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

        # Add labels
        ax.set_xlabel(&#39;X&#39;)
        ax.set_ylabel(&#39;Y&#39;)
        ax.set_zlabel(&#39;Z&#39;)

        ax.set_title(f&#34;Atom Positions Epoch {self.current_epoch:03d} Batch {batch:03d}&#34;)

        # Fix min, max of every coordinate of the true positions
        min_x = min([x[0] for x in true_positions])
        max_x = max([x[0] for x in true_positions])
        min_y = min([x[1] for x in true_positions])
        max_y = max([x[1] for x in true_positions])
        min_z = min([x[2] for x in true_positions])
        max_z = max([x[2] for x in true_positions])

        # Make size fixed with padding
        padding_factor = 0.1
        scale_factor = 1
        fixed_padding = 0.1

        ax.set_xlim(scale_factor * min_x - padding_factor * (max_x - min_x) - fixed_padding, scale_factor * max_x + padding_factor * (max_x - min_x) + fixed_padding)
        ax.set_ylim(scale_factor * min_y - padding_factor * (max_y - min_y) - fixed_padding, scale_factor * max_y + padding_factor * (max_y - min_y) + fixed_padding)
        ax.set_zlim(scale_factor * min_z - padding_factor * (max_z - min_z) - fixed_padding, scale_factor * max_z + padding_factor * (max_z - min_z) + fixed_padding)

        # Make angle fixed from 45 in x and 45 in y
        ax.view_init(azim=-40, elev=35)

        # Create directory if it does not exist
        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;))

        # Find the max number of the files in the folder
        index = len(os.listdir(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)))

        # Save the figure
        # TODO: Fix save folder for atom specific models
        fig.savefig(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;, f&#34;batch_{index}.png&#34;))

    def live_track_test_samle(self, batch, logs=None):
        &#34;&#34;&#34;
        Does the same as track_test_samle but live. This is not working yet.

        Args:
            batch (int): The current batch
            logs (dict, optional): The logs from the training. Defaults to None.
        &#34;&#34;&#34;

        # Predict the output
        pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
        true = self.test_sample[1][0, :, :, 0]

        # Make 3D scatter
        pred_positions = []
        true_positions = []
        for i, bond in enumerate(pred):
            pred_positions.append(pred[i, :])
        for i, bond in enumerate(true):
            true_positions.append(true[i, :])

        self.ax.clear()
        self.ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
        self.ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

        # Add legend
        self.ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

        # Add labels
        self.ax.set_xlabel(&#39;X&#39;)
        self.ax.set_ylabel(&#39;Y&#39;)
        self.ax.set_zlabel(&#39;Z&#39;)
        self.ax.set_title(f&#34;Atom Positions Batch {batch}&#34;)

        # Fix min, max of every coordinate of the true positions
        min_x = min([x[0] for x in true_positions])
        max_x = max([x[0] for x in true_positions])
        min_y = min([x[1] for x in true_positions])
        max_y = max([x[1] for x in true_positions])
        min_z = min([x[2] for x in true_positions])
        max_z = max([x[2] for x in true_positions])

        # Make size fixed with padding
        padding_factor = 0
        self.ax.set_xlim(min_x - padding_factor * (max_x - min_x), max_x + padding_factor * (max_x - min_x))
        self.ax.set_ylim(min_y - padding_factor * (max_y - min_y), max_y + padding_factor * (max_y - min_y))
        self.ax.set_zlim(min_z - padding_factor * (max_z - min_z), max_z + padding_factor * (max_z - min_z))

        # Make angle fixed from 45 in x and 45 in y
        self.ax.view_init(azim=-45, elev=35)

        # Update the plot
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()

    def update_internals(self, epoch, logs):
        &#34;&#34;&#34;
        Update the current epoch and in the future other stuff like learning rate etc.

        Args:
            epoch (int): The current epoch
            logs (dict): Dict with the logs from the training.
        &#34;&#34;&#34;
        self.current_epoch = epoch

    def custom_tensorboard_metrics(self, batch, logs):
        &#34;&#34;&#34;
            Add custom metrics to tensorboard for each batch. This is not working correctly yet.
            
            Args:
                batch (int): The current batch
                logs (dict): Dict with the logs from the training.
        &#34;&#34;&#34;
        # Add custom metrics to tensorboard for each batch

        # Write loss to tensorboard
        summary_dir1 = os.path.join(&#34;data&#34;, &#34;tensorboard&#34;, self.display_name, &#34;custom&#34;)
        summary_writer1 = tf.summary.create_file_writer(summary_dir1)

        # Set step
        step = batch + 148 * batch  # TODO: Fix this

        with summary_writer1.as_default():
            tf.summary.scalar(&#34;loss_b&#34;, logs[&#34;loss&#34;], step=step)
            tf.summary.scalar(&#34;accuracy_b&#34;, logs[&#34;accuracy&#34;], step=step)
            tf.summary.scalar(&#34;mae_b&#34;, logs[&#34;mae&#34;], step=step)

    def test(self, data_generator):
        &#34;&#34;&#34;
            Test the model with a test data generator
        &#34;&#34;&#34;
        X = data_generator.__getitem__(0)[0]
        Y = data_generator.__getitem__(0)[1]
        loss, acc = self.model.evaluate(X, Y, verbose=0)
        print(f&#34;CNN-Test: acc = {100*acc:5.2f}%, loss = {loss:7.4f}&#34;)

    def find_dataset_with_lowest_loss(self, data_generator):
        &#34;&#34;&#34;
            Finds the dataset with the lowest loss
        &#34;&#34;&#34;
        lowest_loss = 100000
        lowest_loss_dataset = None
        for i in range(data_generator.__len__()):
            X = data_generator.__getitem__(i)[0]
            Y = data_generator.__getitem__(i)[1]
            loss, acc = self.model.evaluate(X, Y, verbose=0)
            if loss &lt; lowest_loss:
                lowest_loss = loss
                lowest_loss_dataset = i
        return lowest_loss_dataset

    def predict(self, x):
        &#34;&#34;&#34;
            Predicts the output for a given input
        &#34;&#34;&#34;
        return self.model.predict(x)

    def activation(self, x, layer_name):
        &#34;&#34;&#34;
            Returns the activation map of a given input for a given layer
        &#34;&#34;&#34;
        for layer in self.model.layers:
            if layer.name == layer_name:
                return layer.call(x)
            else:
                x = layer.call(x)

    def save(self, path=None):
        &#34;&#34;&#34;
            Saves the model
        &#34;&#34;&#34;
        path = self.load_path if path is None else path
        self.model.save(path)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="library.classes.models.CNN"><code class="flex name class">
<span>class <span class="ident">CNN</span></span>
<span>(</span><span>input_size: tuple, output_size: tuple, display_name: str, data_prefix: str, keep_checkpoints: bool = False, load_path: str = None, loss: keras.losses.Loss = &lt;keras.losses.MeanSquaredError object&gt;, test_sample: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the base class for all CNNs. It contains the basic structure of the CNN and the fit function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The size of the input. Should be (x, y, 1)</dd>
<dt><strong><code>output_size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The size of the output. Should be (x, y, 1)</dd>
<dt><strong><code>display_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the model. Used for displaying the model summary and saving checkpoints/logs.</dd>
<dt><strong><code>data_prefix</code></strong> :&ensp;<code>str</code></dt>
<dd>The prefix for all data. Used for saving the model and saving tensorboard logs.</dd>
<dt><strong><code>keep_checkpoints</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If true checkpoints will be kept. Defaults to False.</dd>
<dt><strong><code>load_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to the weights to load. Defaults to None.</dd>
<dt><strong><code>loss</code></strong> :&ensp;<code>tf.keras.losses.Loss</code>, optional</dt>
<dd>The loss function to use. Defaults to tf.keras.losses.MeanSquaredError().</dd>
<dt><strong><code>test_sample</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>The test sample to track after each epoch. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CNN:
    def __init__(
        self,
        input_size: tuple,
        output_size: tuple,
        display_name: str,
        data_prefix: str,
        keep_checkpoints: bool = False,
        load_path: str = None,
        loss: tf.keras.losses.Loss = tf.keras.losses.MeanSquaredError(),
        test_sample: tuple = None,
    ):
        &#34;&#34;&#34;
        This is the base class for all CNNs. It contains the basic structure of the CNN and the fit function.

        Args:
            input_size (tuple): The size of the input. Should be (x, y, 1)
            output_size (tuple): The size of the output. Should be (x, y, 1)
            display_name (str): The name of the model. Used for displaying the model summary and saving checkpoints/logs.
            data_prefix (str): The prefix for all data. Used for saving the model and saving tensorboard logs.
            keep_checkpoints (bool, optional): If true checkpoints will be kept. Defaults to False.
            load_path (str, optional): The path to the weights to load. Defaults to None.
            loss (tf.keras.losses.Loss, optional): The loss function to use. Defaults to tf.keras.losses.MeanSquaredError().
            test_sample (tuple, optional): The test sample to track after each epoch. Defaults to None.
        &#34;&#34;&#34;
        super().__init__()

        self.display_name = display_name
        self.keep_checkpoints = keep_checkpoints
        self.load_path = load_path
        self.data_prefix = data_prefix
        self.loss = loss
        self.test_sample = test_sample
        self.current_epoch = 0

        # For showing samples
        self.fig = plt.figure()
        self.ax = self.fig.add_subplot(111, projection=&#39;3d&#39;)

        # Scale the model, this currently only affects the number of filters
        scale = 32          # Scaled the filter dimensions by this factor

        # The activation function to use for the convolutional layers
        conv_activation = tf.keras.layers.LeakyReLU(alpha=0.01)

        self.model = tf.keras.Sequential(
            [
                ##### Input layer #####
                tf.keras.layers.Input(input_size, sparse=False),
                ##### Encoder #####
                tf.keras.layers.Conv2D(
                    filters=2**0 * scale,
                    kernel_size=(2, 2),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**1 * scale,
                    kernel_size=(2, 2),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**2 * scale,
                    kernel_size=(2, 1),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**3 * scale,
                    kernel_size=(2, 1),
                    strides=(1, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**4 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**5 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**6 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.Conv2D(
                    filters=2**7 * scale,
                    kernel_size=(2, 1),
                    strides=(2, 1),
                    padding=&#39;valid&#39;,
                    activation=conv_activation,
                ),
                tf.keras.layers.MaxPool2D(
                    pool_size=(3, 3),
                ),
                tf.keras.layers.BatchNormalization(),
                ##### Output #####
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(np.prod(output_size), activation=&#39;tanh&#39;, kernel_initializer=tf.keras.initializers.Zeros()),  # tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1)),
                tf.keras.layers.Reshape(output_size)
            ], name=self.display_name)

        # Compile the model
        self.model.compile(
            optimizer=tf.optimizers.Adam(
                # learning_rate=0.00001, # Start with 1% of the default learning rate
            ),
            loss=self.loss,
            metrics=[&#39;accuracy&#39;, &#39;mae&#39;],
            run_eagerly=True
        )

        self.model.summary()

        # Load weights if path is given
        if load_path is not None and os.path.exists(load_path):
            self.model.load_weights(load_path)
            print(&#34;Loaded weights from &#34; + load_path)

    def fit(self,
            train_generator,
            validation_gen,
            epochs=150,
            batch_size=64,
            verbose=1,
            early_stop=False,
            ):
        &#34;&#34;&#34;
            Trains the model with the given data

            Args:
                train_generator (tf.keras.utils.Sequence): The training data generator
                validation_gen (tf.keras.utils.Sequence): The validation data generator
                epochs (int, optional): The number of epochs to train. Defaults to 150.
                batch_size (int, optional): The batch size. Defaults to 64.
                verbose (int, optional): The verbosity level. Defaults to 1.
                early_stop (bool, optional): If true early stop will be used. Defaults to False.
        &#34;&#34;&#34;

        # Create hist dir if it does not exist
        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

        callbacks = [
            # The BackupAndRestore callback saves checkpoints every save_freq batches,
            # this is set to 1 epoch here to save after each epoch. Make sure to set
            # this to a value greater than one when training on a large dataset, because
            # it can take a long time to save checkpoints.
            tf.keras.callbacks.experimental.BackupAndRestore(
                backup_dir=os.path.join(self.data_prefix, &#34;backup&#34;, self.display_name),
                # save_freq=1,
                delete_checkpoint=not self.keep_checkpoints,
                # save_before_preemption=False,
            ),

            # The ReduceLROnPlateau callback monitors a quantity and if no improvement
            # is seen for a &#39;patience&#39; number of epochs, the learning rate is reduced.
            # Here, it monitors &#39;val_loss&#39; and if no improvement is seen for 10 epochs,
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor=&#39;val_loss&#39;,
                factor=0.75,
                patience=3,
                verbose=0,
                mode=&#39;max&#39;,
                min_delta=0.00000005,
                cooldown=5,
                min_lr=0.0000000000001,
            ),

            # The CSVLogger callback streams epoch results to a CSV file.
            # #TODO Send the CSV file via email or telegram after each epoch
            # to monitor the training progress.
            tf.keras.callbacks.CSVLogger(
                os.path.join(self.data_prefix, &#34;hist&#34;, f&#34;training_history_{self.display_name}.csv&#34;),
                separator=&#39;,&#39;,
                append=True,
            ),

            # The TensorBoard callback writes a log for TensorBoard, which allows
            # you to visualize dynamic graphs of your training and test metrics,
            # as well as activation histograms for the different layers in your model.
            tf.keras.callbacks.TensorBoard(
                log_dir=os.path.join(self.data_prefix, &#39;tensorboard&#39;, self.display_name),
                histogram_freq=1,
                write_graph=True,
                write_images=True,
                update_freq=&#39;batch&#39;,
            ),
            # Update the current epoch and in the future other stuff
            tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: self.update_internals(epoch, logs)),
            # Track the test sample after each epoch
            # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda epoch, logs: self.track_test_samle(epoch, logs)),
            # Track the test sample after each batch (live)
            # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.live_track_test_samle(batch, logs)),
            # Add custom metrics to tensorboard
            tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.custom_tensorboard_metrics(batch, logs)),
        ]

        if early_stop:
            # The EarlyStopping callback is used to exit the training early if
            # the validation loss does not improve after 20 epochs.
            # This makes sure that the model does not overfit the training data.
            callbacks.append(
                tf.keras.callbacks.EarlyStopping(
                    monitor=&#39;val_loss&#39;,
                    min_delta=0.0001,
                    patience=20,
                    verbose=0,
                    mode=&#39;auto&#39;,
                    baseline=None,
                    restore_best_weights=True,
                ))

        # Open plot for live tracking TODO: fix this
        # self.fig.show()
        # self.fig.canvas.draw()

        # Here we use generators to generate batches of data for the training.
        # The training data is currently generated by the RelativeVectorsTrainingDataGenerator
        # and is the main thing that changes the input data structure to the desired output.
        self.hist = self.model.fit(
            train_generator,
            validation_data=validation_gen,
            batch_size=batch_size,
            shuffle=True,
            epochs=epochs,
            verbose=verbose,
            callbacks=callbacks,
        )

    def track_test_samle(self, batch, logs=None):
        &#34;&#34;&#34;
            Tracks the test sample after each batch and saves the figure to the hist folder.

            Args:
                batch (int): The current batch
                logs (dict, optional): The logs from the training. Defaults to None.
        &#34;&#34;&#34;

        # Predict the output
        pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
        true = self.test_sample[1][0, :, :, 0]

        # Remove padding
        pred = pred[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]
        true = true[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]

        # Make 3D scatter
        pred_positions = []
        true_positions = []
        for i, bond in enumerate(pred):
            pred_positions.append(pred[i, :])
        for i, bond in enumerate(true):
            true_positions.append(true[i, :])

        fig = plt.figure()
        ax = fig.add_subplot(111, projection=&#39;3d&#39;)

        ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
        ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

        # Draw lines between the atoms predicted and true positions
        for i in range(len(true_positions)):
            ax.plot([true_positions[i][0], pred_positions[i][0]], [true_positions[i][1], pred_positions[i][1]], [true_positions[i][2], pred_positions[i][2]], color=&#39;gray&#39;, linestyle=&#39;-&#39;, linewidth=0.1)

        # Add legend
        ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

        # Add labels
        ax.set_xlabel(&#39;X&#39;)
        ax.set_ylabel(&#39;Y&#39;)
        ax.set_zlabel(&#39;Z&#39;)

        ax.set_title(f&#34;Atom Positions Epoch {self.current_epoch:03d} Batch {batch:03d}&#34;)

        # Fix min, max of every coordinate of the true positions
        min_x = min([x[0] for x in true_positions])
        max_x = max([x[0] for x in true_positions])
        min_y = min([x[1] for x in true_positions])
        max_y = max([x[1] for x in true_positions])
        min_z = min([x[2] for x in true_positions])
        max_z = max([x[2] for x in true_positions])

        # Make size fixed with padding
        padding_factor = 0.1
        scale_factor = 1
        fixed_padding = 0.1

        ax.set_xlim(scale_factor * min_x - padding_factor * (max_x - min_x) - fixed_padding, scale_factor * max_x + padding_factor * (max_x - min_x) + fixed_padding)
        ax.set_ylim(scale_factor * min_y - padding_factor * (max_y - min_y) - fixed_padding, scale_factor * max_y + padding_factor * (max_y - min_y) + fixed_padding)
        ax.set_zlim(scale_factor * min_z - padding_factor * (max_z - min_z) - fixed_padding, scale_factor * max_z + padding_factor * (max_z - min_z) + fixed_padding)

        # Make angle fixed from 45 in x and 45 in y
        ax.view_init(azim=-40, elev=35)

        # Create directory if it does not exist
        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

        if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)):
            os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;))

        # Find the max number of the files in the folder
        index = len(os.listdir(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)))

        # Save the figure
        # TODO: Fix save folder for atom specific models
        fig.savefig(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;, f&#34;batch_{index}.png&#34;))

    def live_track_test_samle(self, batch, logs=None):
        &#34;&#34;&#34;
        Does the same as track_test_samle but live. This is not working yet.

        Args:
            batch (int): The current batch
            logs (dict, optional): The logs from the training. Defaults to None.
        &#34;&#34;&#34;

        # Predict the output
        pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
        true = self.test_sample[1][0, :, :, 0]

        # Make 3D scatter
        pred_positions = []
        true_positions = []
        for i, bond in enumerate(pred):
            pred_positions.append(pred[i, :])
        for i, bond in enumerate(true):
            true_positions.append(true[i, :])

        self.ax.clear()
        self.ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
        self.ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

        # Add legend
        self.ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

        # Add labels
        self.ax.set_xlabel(&#39;X&#39;)
        self.ax.set_ylabel(&#39;Y&#39;)
        self.ax.set_zlabel(&#39;Z&#39;)
        self.ax.set_title(f&#34;Atom Positions Batch {batch}&#34;)

        # Fix min, max of every coordinate of the true positions
        min_x = min([x[0] for x in true_positions])
        max_x = max([x[0] for x in true_positions])
        min_y = min([x[1] for x in true_positions])
        max_y = max([x[1] for x in true_positions])
        min_z = min([x[2] for x in true_positions])
        max_z = max([x[2] for x in true_positions])

        # Make size fixed with padding
        padding_factor = 0
        self.ax.set_xlim(min_x - padding_factor * (max_x - min_x), max_x + padding_factor * (max_x - min_x))
        self.ax.set_ylim(min_y - padding_factor * (max_y - min_y), max_y + padding_factor * (max_y - min_y))
        self.ax.set_zlim(min_z - padding_factor * (max_z - min_z), max_z + padding_factor * (max_z - min_z))

        # Make angle fixed from 45 in x and 45 in y
        self.ax.view_init(azim=-45, elev=35)

        # Update the plot
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()

    def update_internals(self, epoch, logs):
        &#34;&#34;&#34;
        Update the current epoch and in the future other stuff like learning rate etc.

        Args:
            epoch (int): The current epoch
            logs (dict): Dict with the logs from the training.
        &#34;&#34;&#34;
        self.current_epoch = epoch

    def custom_tensorboard_metrics(self, batch, logs):
        &#34;&#34;&#34;
            Add custom metrics to tensorboard for each batch. This is not working correctly yet.
            
            Args:
                batch (int): The current batch
                logs (dict): Dict with the logs from the training.
        &#34;&#34;&#34;
        # Add custom metrics to tensorboard for each batch

        # Write loss to tensorboard
        summary_dir1 = os.path.join(&#34;data&#34;, &#34;tensorboard&#34;, self.display_name, &#34;custom&#34;)
        summary_writer1 = tf.summary.create_file_writer(summary_dir1)

        # Set step
        step = batch + 148 * batch  # TODO: Fix this

        with summary_writer1.as_default():
            tf.summary.scalar(&#34;loss_b&#34;, logs[&#34;loss&#34;], step=step)
            tf.summary.scalar(&#34;accuracy_b&#34;, logs[&#34;accuracy&#34;], step=step)
            tf.summary.scalar(&#34;mae_b&#34;, logs[&#34;mae&#34;], step=step)

    def test(self, data_generator):
        &#34;&#34;&#34;
            Test the model with a test data generator
        &#34;&#34;&#34;
        X = data_generator.__getitem__(0)[0]
        Y = data_generator.__getitem__(0)[1]
        loss, acc = self.model.evaluate(X, Y, verbose=0)
        print(f&#34;CNN-Test: acc = {100*acc:5.2f}%, loss = {loss:7.4f}&#34;)

    def find_dataset_with_lowest_loss(self, data_generator):
        &#34;&#34;&#34;
            Finds the dataset with the lowest loss
        &#34;&#34;&#34;
        lowest_loss = 100000
        lowest_loss_dataset = None
        for i in range(data_generator.__len__()):
            X = data_generator.__getitem__(i)[0]
            Y = data_generator.__getitem__(i)[1]
            loss, acc = self.model.evaluate(X, Y, verbose=0)
            if loss &lt; lowest_loss:
                lowest_loss = loss
                lowest_loss_dataset = i
        return lowest_loss_dataset

    def predict(self, x):
        &#34;&#34;&#34;
            Predicts the output for a given input
        &#34;&#34;&#34;
        return self.model.predict(x)

    def activation(self, x, layer_name):
        &#34;&#34;&#34;
            Returns the activation map of a given input for a given layer
        &#34;&#34;&#34;
        for layer in self.model.layers:
            if layer.name == layer_name:
                return layer.call(x)
            else:
                x = layer.call(x)

    def save(self, path=None):
        &#34;&#34;&#34;
            Saves the model
        &#34;&#34;&#34;
        path = self.load_path if path is None else path
        self.model.save(path)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="library.classes.models.CNN.activation"><code class="name flex">
<span>def <span class="ident">activation</span></span>(<span>self, x, layer_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the activation map of a given input for a given layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def activation(self, x, layer_name):
    &#34;&#34;&#34;
        Returns the activation map of a given input for a given layer
    &#34;&#34;&#34;
    for layer in self.model.layers:
        if layer.name == layer_name:
            return layer.call(x)
        else:
            x = layer.call(x)</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.custom_tensorboard_metrics"><code class="name flex">
<span>def <span class="ident">custom_tensorboard_metrics</span></span>(<span>self, batch, logs)</span>
</code></dt>
<dd>
<div class="desc"><p>Add custom metrics to tensorboard for each batch. This is not working correctly yet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>int</code></dt>
<dd>The current batch</dd>
<dt><strong><code>logs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict with the logs from the training.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def custom_tensorboard_metrics(self, batch, logs):
    &#34;&#34;&#34;
        Add custom metrics to tensorboard for each batch. This is not working correctly yet.
        
        Args:
            batch (int): The current batch
            logs (dict): Dict with the logs from the training.
    &#34;&#34;&#34;
    # Add custom metrics to tensorboard for each batch

    # Write loss to tensorboard
    summary_dir1 = os.path.join(&#34;data&#34;, &#34;tensorboard&#34;, self.display_name, &#34;custom&#34;)
    summary_writer1 = tf.summary.create_file_writer(summary_dir1)

    # Set step
    step = batch + 148 * batch  # TODO: Fix this

    with summary_writer1.as_default():
        tf.summary.scalar(&#34;loss_b&#34;, logs[&#34;loss&#34;], step=step)
        tf.summary.scalar(&#34;accuracy_b&#34;, logs[&#34;accuracy&#34;], step=step)
        tf.summary.scalar(&#34;mae_b&#34;, logs[&#34;mae&#34;], step=step)</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.find_dataset_with_lowest_loss"><code class="name flex">
<span>def <span class="ident">find_dataset_with_lowest_loss</span></span>(<span>self, data_generator)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the dataset with the lowest loss</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_dataset_with_lowest_loss(self, data_generator):
    &#34;&#34;&#34;
        Finds the dataset with the lowest loss
    &#34;&#34;&#34;
    lowest_loss = 100000
    lowest_loss_dataset = None
    for i in range(data_generator.__len__()):
        X = data_generator.__getitem__(i)[0]
        Y = data_generator.__getitem__(i)[1]
        loss, acc = self.model.evaluate(X, Y, verbose=0)
        if loss &lt; lowest_loss:
            lowest_loss = loss
            lowest_loss_dataset = i
    return lowest_loss_dataset</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, train_generator, validation_gen, epochs=150, batch_size=64, verbose=1, early_stop=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the model with the given data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_generator</code></strong> :&ensp;<code>tf.keras.utils.Sequence</code></dt>
<dd>The training data generator</dd>
<dt><strong><code>validation_gen</code></strong> :&ensp;<code>tf.keras.utils.Sequence</code></dt>
<dd>The validation data generator</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs to train. Defaults to 150.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The batch size. Defaults to 64.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The verbosity level. Defaults to 1.</dd>
<dt><strong><code>early_stop</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If true early stop will be used. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self,
        train_generator,
        validation_gen,
        epochs=150,
        batch_size=64,
        verbose=1,
        early_stop=False,
        ):
    &#34;&#34;&#34;
        Trains the model with the given data

        Args:
            train_generator (tf.keras.utils.Sequence): The training data generator
            validation_gen (tf.keras.utils.Sequence): The validation data generator
            epochs (int, optional): The number of epochs to train. Defaults to 150.
            batch_size (int, optional): The batch size. Defaults to 64.
            verbose (int, optional): The verbosity level. Defaults to 1.
            early_stop (bool, optional): If true early stop will be used. Defaults to False.
    &#34;&#34;&#34;

    # Create hist dir if it does not exist
    if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
        os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

    callbacks = [
        # The BackupAndRestore callback saves checkpoints every save_freq batches,
        # this is set to 1 epoch here to save after each epoch. Make sure to set
        # this to a value greater than one when training on a large dataset, because
        # it can take a long time to save checkpoints.
        tf.keras.callbacks.experimental.BackupAndRestore(
            backup_dir=os.path.join(self.data_prefix, &#34;backup&#34;, self.display_name),
            # save_freq=1,
            delete_checkpoint=not self.keep_checkpoints,
            # save_before_preemption=False,
        ),

        # The ReduceLROnPlateau callback monitors a quantity and if no improvement
        # is seen for a &#39;patience&#39; number of epochs, the learning rate is reduced.
        # Here, it monitors &#39;val_loss&#39; and if no improvement is seen for 10 epochs,
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor=&#39;val_loss&#39;,
            factor=0.75,
            patience=3,
            verbose=0,
            mode=&#39;max&#39;,
            min_delta=0.00000005,
            cooldown=5,
            min_lr=0.0000000000001,
        ),

        # The CSVLogger callback streams epoch results to a CSV file.
        # #TODO Send the CSV file via email or telegram after each epoch
        # to monitor the training progress.
        tf.keras.callbacks.CSVLogger(
            os.path.join(self.data_prefix, &#34;hist&#34;, f&#34;training_history_{self.display_name}.csv&#34;),
            separator=&#39;,&#39;,
            append=True,
        ),

        # The TensorBoard callback writes a log for TensorBoard, which allows
        # you to visualize dynamic graphs of your training and test metrics,
        # as well as activation histograms for the different layers in your model.
        tf.keras.callbacks.TensorBoard(
            log_dir=os.path.join(self.data_prefix, &#39;tensorboard&#39;, self.display_name),
            histogram_freq=1,
            write_graph=True,
            write_images=True,
            update_freq=&#39;batch&#39;,
        ),
        # Update the current epoch and in the future other stuff
        tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: self.update_internals(epoch, logs)),
        # Track the test sample after each epoch
        # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda epoch, logs: self.track_test_samle(epoch, logs)),
        # Track the test sample after each batch (live)
        # tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.live_track_test_samle(batch, logs)),
        # Add custom metrics to tensorboard
        tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.custom_tensorboard_metrics(batch, logs)),
    ]

    if early_stop:
        # The EarlyStopping callback is used to exit the training early if
        # the validation loss does not improve after 20 epochs.
        # This makes sure that the model does not overfit the training data.
        callbacks.append(
            tf.keras.callbacks.EarlyStopping(
                monitor=&#39;val_loss&#39;,
                min_delta=0.0001,
                patience=20,
                verbose=0,
                mode=&#39;auto&#39;,
                baseline=None,
                restore_best_weights=True,
            ))

    # Open plot for live tracking TODO: fix this
    # self.fig.show()
    # self.fig.canvas.draw()

    # Here we use generators to generate batches of data for the training.
    # The training data is currently generated by the RelativeVectorsTrainingDataGenerator
    # and is the main thing that changes the input data structure to the desired output.
    self.hist = self.model.fit(
        train_generator,
        validation_data=validation_gen,
        batch_size=batch_size,
        shuffle=True,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
    )</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.live_track_test_samle"><code class="name flex">
<span>def <span class="ident">live_track_test_samle</span></span>(<span>self, batch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Does the same as track_test_samle but live. This is not working yet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>int</code></dt>
<dd>The current batch</dd>
<dt><strong><code>logs</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>The logs from the training. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def live_track_test_samle(self, batch, logs=None):
    &#34;&#34;&#34;
    Does the same as track_test_samle but live. This is not working yet.

    Args:
        batch (int): The current batch
        logs (dict, optional): The logs from the training. Defaults to None.
    &#34;&#34;&#34;

    # Predict the output
    pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
    true = self.test_sample[1][0, :, :, 0]

    # Make 3D scatter
    pred_positions = []
    true_positions = []
    for i, bond in enumerate(pred):
        pred_positions.append(pred[i, :])
    for i, bond in enumerate(true):
        true_positions.append(true[i, :])

    self.ax.clear()
    self.ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
    self.ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

    # Add legend
    self.ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

    # Add labels
    self.ax.set_xlabel(&#39;X&#39;)
    self.ax.set_ylabel(&#39;Y&#39;)
    self.ax.set_zlabel(&#39;Z&#39;)
    self.ax.set_title(f&#34;Atom Positions Batch {batch}&#34;)

    # Fix min, max of every coordinate of the true positions
    min_x = min([x[0] for x in true_positions])
    max_x = max([x[0] for x in true_positions])
    min_y = min([x[1] for x in true_positions])
    max_y = max([x[1] for x in true_positions])
    min_z = min([x[2] for x in true_positions])
    max_z = max([x[2] for x in true_positions])

    # Make size fixed with padding
    padding_factor = 0
    self.ax.set_xlim(min_x - padding_factor * (max_x - min_x), max_x + padding_factor * (max_x - min_x))
    self.ax.set_ylim(min_y - padding_factor * (max_y - min_y), max_y + padding_factor * (max_y - min_y))
    self.ax.set_zlim(min_z - padding_factor * (max_z - min_z), max_z + padding_factor * (max_z - min_z))

    # Make angle fixed from 45 in x and 45 in y
    self.ax.view_init(azim=-45, elev=35)

    # Update the plot
    self.fig.canvas.draw()
    self.fig.canvas.flush_events()</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Predicts the output for a given input</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x):
    &#34;&#34;&#34;
        Predicts the output for a given input
    &#34;&#34;&#34;
    return self.model.predict(x)</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path=None):
    &#34;&#34;&#34;
        Saves the model
    &#34;&#34;&#34;
    path = self.load_path if path is None else path
    self.model.save(path)</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, data_generator)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the model with a test data generator</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self, data_generator):
    &#34;&#34;&#34;
        Test the model with a test data generator
    &#34;&#34;&#34;
    X = data_generator.__getitem__(0)[0]
    Y = data_generator.__getitem__(0)[1]
    loss, acc = self.model.evaluate(X, Y, verbose=0)
    print(f&#34;CNN-Test: acc = {100*acc:5.2f}%, loss = {loss:7.4f}&#34;)</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.track_test_samle"><code class="name flex">
<span>def <span class="ident">track_test_samle</span></span>(<span>self, batch, logs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Tracks the test sample after each batch and saves the figure to the hist folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>int</code></dt>
<dd>The current batch</dd>
<dt><strong><code>logs</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>The logs from the training. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def track_test_samle(self, batch, logs=None):
    &#34;&#34;&#34;
        Tracks the test sample after each batch and saves the figure to the hist folder.

        Args:
            batch (int): The current batch
            logs (dict, optional): The logs from the training. Defaults to None.
    &#34;&#34;&#34;

    # Predict the output
    pred = self.model.predict(self.test_sample[0][0:1, :, :, :])[0, :, :, 0]
    true = self.test_sample[1][0, :, :, 0]

    # Remove padding
    pred = pred[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]
    true = true[PADDING_X:-PADDING_X, PADDING_Y:-PADDING_Y]

    # Make 3D scatter
    pred_positions = []
    true_positions = []
    for i, bond in enumerate(pred):
        pred_positions.append(pred[i, :])
    for i, bond in enumerate(true):
        true_positions.append(true[i, :])

    fig = plt.figure()
    ax = fig.add_subplot(111, projection=&#39;3d&#39;)

    ax.scatter([x[0] for x in true_positions], [x[1] for x in true_positions], [x[2] for x in true_positions], c=&#39;b&#39;, marker=&#39;o&#39;)
    ax.scatter([x[0] for x in pred_positions], [x[1] for x in pred_positions], [x[2] for x in pred_positions], c=&#39;r&#39;, marker=&#39;o&#39;)

    # Draw lines between the atoms predicted and true positions
    for i in range(len(true_positions)):
        ax.plot([true_positions[i][0], pred_positions[i][0]], [true_positions[i][1], pred_positions[i][1]], [true_positions[i][2], pred_positions[i][2]], color=&#39;gray&#39;, linestyle=&#39;-&#39;, linewidth=0.1)

    # Add legend
    ax.legend([&#34;True&#34;, &#34;Predicted&#34;])

    # Add labels
    ax.set_xlabel(&#39;X&#39;)
    ax.set_ylabel(&#39;Y&#39;)
    ax.set_zlabel(&#39;Z&#39;)

    ax.set_title(f&#34;Atom Positions Epoch {self.current_epoch:03d} Batch {batch:03d}&#34;)

    # Fix min, max of every coordinate of the true positions
    min_x = min([x[0] for x in true_positions])
    max_x = max([x[0] for x in true_positions])
    min_y = min([x[1] for x in true_positions])
    max_y = max([x[1] for x in true_positions])
    min_z = min([x[2] for x in true_positions])
    max_z = max([x[2] for x in true_positions])

    # Make size fixed with padding
    padding_factor = 0.1
    scale_factor = 1
    fixed_padding = 0.1

    ax.set_xlim(scale_factor * min_x - padding_factor * (max_x - min_x) - fixed_padding, scale_factor * max_x + padding_factor * (max_x - min_x) + fixed_padding)
    ax.set_ylim(scale_factor * min_y - padding_factor * (max_y - min_y) - fixed_padding, scale_factor * max_y + padding_factor * (max_y - min_y) + fixed_padding)
    ax.set_zlim(scale_factor * min_z - padding_factor * (max_z - min_z) - fixed_padding, scale_factor * max_z + padding_factor * (max_z - min_z) + fixed_padding)

    # Make angle fixed from 45 in x and 45 in y
    ax.view_init(azim=-40, elev=35)

    # Create directory if it does not exist
    if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;)):
        os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;))

    if not os.path.exists(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)):
        os.makedirs(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;))

    # Find the max number of the files in the folder
    index = len(os.listdir(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;)))

    # Save the figure
    # TODO: Fix save folder for atom specific models
    fig.savefig(os.path.join(self.data_prefix, &#34;hist&#34;, &#34;pred&#34;, f&#34;batch_{index}.png&#34;))</code></pre>
</details>
</dd>
<dt id="library.classes.models.CNN.update_internals"><code class="name flex">
<span>def <span class="ident">update_internals</span></span>(<span>self, epoch, logs)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the current epoch and in the future other stuff like learning rate etc.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>The current epoch</dd>
<dt><strong><code>logs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dict with the logs from the training.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_internals(self, epoch, logs):
    &#34;&#34;&#34;
    Update the current epoch and in the future other stuff like learning rate etc.

    Args:
        epoch (int): The current epoch
        logs (dict): Dict with the logs from the training.
    &#34;&#34;&#34;
    self.current_epoch = epoch</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="library.classes" href="index.html">library.classes</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="library.classes.models.CNN" href="#library.classes.models.CNN">CNN</a></code></h4>
<ul class="">
<li><code><a title="library.classes.models.CNN.activation" href="#library.classes.models.CNN.activation">activation</a></code></li>
<li><code><a title="library.classes.models.CNN.custom_tensorboard_metrics" href="#library.classes.models.CNN.custom_tensorboard_metrics">custom_tensorboard_metrics</a></code></li>
<li><code><a title="library.classes.models.CNN.find_dataset_with_lowest_loss" href="#library.classes.models.CNN.find_dataset_with_lowest_loss">find_dataset_with_lowest_loss</a></code></li>
<li><code><a title="library.classes.models.CNN.fit" href="#library.classes.models.CNN.fit">fit</a></code></li>
<li><code><a title="library.classes.models.CNN.live_track_test_samle" href="#library.classes.models.CNN.live_track_test_samle">live_track_test_samle</a></code></li>
<li><code><a title="library.classes.models.CNN.predict" href="#library.classes.models.CNN.predict">predict</a></code></li>
<li><code><a title="library.classes.models.CNN.save" href="#library.classes.models.CNN.save">save</a></code></li>
<li><code><a title="library.classes.models.CNN.test" href="#library.classes.models.CNN.test">test</a></code></li>
<li><code><a title="library.classes.models.CNN.track_test_samle" href="#library.classes.models.CNN.track_test_samle">track_test_samle</a></code></li>
<li><code><a title="library.classes.models.CNN.update_internals" href="#library.classes.models.CNN.update_internals">update_internals</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>