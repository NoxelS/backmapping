import os
import tensorflow as tf
import numpy as np
from scipy.ndimage import gaussian_filter
import sys

from library.DataParser import get_cg_at_datasets
from library.static.MartiniMaps import DOPC_CG_NAME_TO_TYPE_MAP, DOPC_BEAD_TYPE_NAME_IDS, DOPC_ELEMENT_TYPE_NAME_IDS
from Bio.PDB.PDBIO import Select
from Bio.PDB.PDBIO import PDBIO
from Bio.PDB.PDBParser import PDBParser


class TrainingDataGenerator(tf.keras.utils.Sequence):
    """
        A data generator class that generates batches of data for the CNN.
        It uses the data generated by the parse-pdb.py script.
    """

    def __init__(self, input_dir_path, output_dir_path, input_size, output_size, shuffle=False, batch_size=1):
        """
            Initializes a data generator object to generate batches of data.
            :param input_dir_path: The path to the directory where the input data (X) is located
            :param output_dir_path: The path to the directory where the output data (Y) is located
            :param input_size: The size/shape of the input data
            :param output_size: The size/shape of the output data
            :param shuffle: If the data should be shuffled after each epoch
            :param batch_size: The size of each batch
            :param atom_types: The atom types that should be used for the output data
        """
        self.input_dir_path = input_dir_path
        self.output_dir_path = output_dir_path
        self.input_size = input_size
        self.output_size = output_size
        self.shuffle = shuffle
        self.batch_size = batch_size

        self.len = os.listdir(input_dir_path).__len__()

        # # Get all CG and AT datasets (this is only indexing the data, not loading it)
        # cg_datasets, at_datasets = get_cg_at_datasets(input_dir_path)

        # io = PDBIO()
        self.parser = PDBParser(QUIET=True)

        # io.set_structure(cg_datasets[0].get_structure())
        # io.save(f"{0}.pdb", ResidueSelector(1), preserve_atom_numbering=True)

        # # Get the res
        # s = parser.get_structure(1, f"{0}.pdb")
        # print(list(s.get_residues().__next__().get_atoms()))

        # idx = 0

        # # Loop over both at the same time (these are generators, so they are not loaded into memory immediately)
        # for i, (cg_dataset, at_dataset) in enumerate(zip(cg_datasets, at_datasets)):
        #     for j, (cg_residue, at_residue) in enumerate(zip(cg_dataset.get_residues(), at_dataset.get_residues())):
        #         # Create folder for the idx
        #         if not os.path.exists(f"{output_dir_path}/{idx}"):
        #             os.makedirs(f"data/training/{idx}")

        #         io.set_structure(cg_dataset.get_structure())
        #         io.save(f"data/training/{idx}/cg.pdb",
        #                 ResidueSelector(j), preserve_atom_numbering=True)

        #         io.set_structure(at_dataset.get_structure())
        #         io.save(f"data/training/{idx}/at.pdb",
        #                 ResidueSelector(j), preserve_atom_numbering=True)

        #         idx += 1

        #         if(idx == 10):
        #             break

        # Debug
        print(
            f"Found {self.len} residues in for training in {self.input_dir_path}")

        # Initialize
        self.on_epoch_end()

    def on_epoch_end(self):
        pass

    def __len__(self):
        return self.len // self.batch_size

    def __getitem__(self, idx):
        # Initialize Batch
        X = np.empty((self.batch_size, *self.input_size))
        Y = np.empty((self.batch_size, *self.output_size))

        # Load the two files in the idx folder
        for i in range(self.batch_size):
            # Get the index of the residue
            residue_idx = idx * self.batch_size + i

            # Get the path to the files
            cg_path = f"{self.input_dir_path}/{residue_idx}/cg.pdb"
            at_path = f"{self.input_dir_path}/{residue_idx}/at.pdb"

            # Load the files
            cg_structure = self.parser.get_structure(residue_idx, cg_path)
            at_structure = self.parser.get_structure(residue_idx, at_path)

            # Get the residues
            cg_residue = list(cg_structure.get_residues())[0]
            at_residue = list(at_structure.get_residues())[0]

            # Get the atoms
            cg_atoms = list(cg_residue.get_atoms())
            at_atoms = list(at_residue.get_atoms())

            # Make a 200x200x200 box for coordinates
            X_MAX = 200
            Y_MAX = 200
            Z_MAX = 200

            # Make the cg data (batchsize, 12, 8)
            for j, bead in enumerate(cg_atoms):
                # Get coordinates
                x, y, z = bead.get_coord()

                # Make the coordinates relative to the box
                X[i, j, 0] = x / X_MAX
                X[i, j, 1] = y / Y_MAX
                X[i, j, 2] = z / Z_MAX

                # Make one hot encoding for the bead type
                X[i, j, 3:8] = 0
                bead_type_id = DOPC_BEAD_TYPE_NAME_IDS[DOPC_CG_NAME_TO_TYPE_MAP[bead.get_name(
                )]]
                X[i, j, 3 + bead_type_id] = 1

            # Make the at data (batchsize, 138, 8)
            for j, atom in enumerate(at_atoms):
                # Get coordinates
                x, y, z = atom.get_coord()

                # Make the coordinates relative to the box
                Y[i, j, 0] = x / X_MAX
                Y[i, j, 1] = y / Y_MAX
                Y[i, j, 2] = z / Z_MAX

                # Make one hot encoding for the bead type
                Y[i, j, 3:8] = 0
                at_type_id = DOPC_ELEMENT_TYPE_NAME_IDS[atom.element]
                Y[i, j, 3 + at_type_id] = 1

        # Convert to tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        Y = tf.convert_to_tensor(Y, dtype=tf.float32)

        # Return tensor
        return X, Y
