import json
import linecache
import os
import pickle
import random
import sys
from ctypes import Structure

import numpy as np
import tensorflow as tf
from Bio.PDB.PDBParser import PDBParser

from library.config import Keys, config
from library.datagen.topology import get_ic_from_index, get_ic_type, ic_to_hlabel, load_extended_topology_info
from library.static.utils import print_input_matrix, print_matrix

# Read datapaths from config
NEIGHBOURHOOD_PATH = os.path.join(config(Keys.DATA_PATH), "neighborhoods.csv")  # This is generated by the gen_train_data.py script
CG_BOUNDING_BOX_RELATION_PATH = os.path.join(config(Keys.DATA_PATH), "box_sizes_cg.csv")  # Path to the csv file that contains the bounding box sizes for the cg residues
AT_BOUNDING_BOX_RELATION_PATH = os.path.join(config(Keys.DATA_PATH), "box_sizes_at.csv")  # This is generated by the gen_train_data.py script


# Read generator config
PADDING = config(Keys.PADDING)  # Padding on the input and output matrices
PBC_CUTOFF = config(Keys.PBC_CUTOFF)  # If a bond is longer than this, it is considered to be on the other side of the simulation box
INPUT_SCALE = config(Keys.INPUT_SCALE)  # This factor is used to scale down bead positions
OUTPUT_SCALE = config(Keys.OUTPUT_SCALE)  # The scale factor to downscale the output matrix (applied after the transformation to [0,1])


def fix_pbc(vector, box_size, cutoff=[PBC_CUTOFF, PBC_CUTOFF, PBC_CUTOFF]):
    """
    This function fixes the periodic boundary conditions of a vector. It does this by checking if the vector is outside of the box and if it is, it will move it back into the box.

    Args:
        vector (vector): The vector that should be fixed.
        box_size (vector, optional): The box size of the simulation.
        cutoff (list, optional): Cutoff radius to apply the PBC fix to. Defaults to [PBC_CUTOFF, PBC_CUTOFF, PBC_CUTOFF].

    Returns:
        vector: The fixed vector.
    """
    if vector[0] > cutoff[0]:
        vector[0] -= box_size[0]
    elif vector[0] < -cutoff[0]:
        vector[0] += box_size[0]
    if vector[1] > cutoff[1]:
        vector[1] -= box_size[1]
    elif vector[1] < -cutoff[1]:
        vector[1] += box_size[1]
    if vector[2] > cutoff[2]:
        vector[2] -= box_size[2]
    elif vector[2] < -cutoff[2]:
        vector[2] += box_size[2]

    if vector[0] > cutoff[0] or vector[0] < -cutoff[0] or vector[1] > cutoff[1] or vector[1] < -cutoff[1] or vector[2] > cutoff[2] or vector[2] < -cutoff[2]:
        return fix_pbc(vector, box_size)
    else:
        return vector


def is_output_matrix_healthy(output, range=[-1, 1]):
    """
    This is a diagnostic function that checks if the output matrix is fulffiling the requirements.
    Checked will be:
        - If values outside of [-1, 1] exist (this should not be the case)
        - If nan or inf exist (this should not be the case)
    TODO: This functions can be extended to check for other things as well.
    """
    healthy = True

    # Check if values that are not in [-1, 1] exist
    if np.max(output) > range[1] or np.min(output) < range[0]:
        healthy = False

    # Check for nan or inf
    if np.isnan(output).any() or np.isinf(output).any():
        healthy = False

    return healthy


def add_absolute_positions(atoms, output_matrix, batch_index, box, target_atom=None, preset_position_origin=None, position_scale=INPUT_SCALE):
    """
    This functions calculates the absolute positions of the atoms in the atom mapping and writes them to the output matrix.

    Args:
        atom_pos_dict (_type_): The atom pos dict is a dict that maps the atom name to the atom position.
        atom_mapping (_type_): The atom mapping is a list of tuples that contain the atom names that should be mapped.
        output_matrix (_type_): The output matrix is the matrix where the relative vectors should be written to. Shape: (batch_size, i, j, 1)
        batch_index (_type_): The batch index is the index of the batch in the output matrix.
        residue_idx (_type_): The residue index is the index of the residue in the dataset.
        target_name (_type_): The target is the atom that should be fitted. If this is None, all atoms will be fitted. If a target is set, only this atom will be in the output matrix.
        preset_position_origin (_type_): The position origin is the position of the atom that should be fitted. If this is None, the position of the N atom or NC3 bead will be used.
    """
    # Filter out hydrogen atoms
    atoms = [atom for atom in atoms if atom.element != "H"]

    # Make base position
    position_origin = [atom for atom in atoms if atom.get_name() == "N" or atom.get_name() == "NC3"][0].get_vector()  # Everything is relative to the N atom or NC3 bead
    if preset_position_origin:
        position_origin = preset_position_origin

    # Remove the N atom or NC3 bead because their position is the base position (0,0,0)
    if not preset_position_origin:
        atoms = [atom for atom in atoms if atom.get_name() != "N" and atom.get_name() != "NC3"]

    if target_atom:
        # If only one atom is selected we need to filter out all other atoms
        atoms = [atom for atom in atoms if atom.get_name() == target_atom.get_name()]

        if len(atoms) != 1:
            raise Exception(f"Found {len(atoms)} atoms with the name {target_atom.get_name()}!")

    for j, atom in enumerate(atoms):
        # Calculate the relative vector
        position = atom.get_vector() - position_origin

        # Fix the PBC
        position = fix_pbc(position, box, cutoff=[box[0] / 1.5, box[1] / 1.5, box[2] / 1.5])  # TODO: Maybe find better cutoff approximations

        # Check if nan or inf is in the vector
        if np.isnan(position[0]) or np.isnan(position[1]) or np.isnan(position[2]) or np.isinf(position[0]) or np.isinf(position[1]) or np.isinf(position[2]):
            raise Exception(f"Found nan or inf in vector ({position})!")

        # TODO: translate to [0,1] here

        # Write the relative vectors to the output matrix
        for k in range(3):
            output_matrix[batch_index, j + PADDING, PADDING + k, 0] = (
                position[k] * position_scale
            )  # <- Position scale factor depends on if the atom is a bead or an atom (beads contain possible neighbors and cover a much larger area!)

    return output_matrix


def get_bounding_box(dataset_idx: int, path_to_csv: str) -> np.array:
    """
    This function returns the bounding box of a residue in a dataset. The bounding box is the size of the simulation box in which the residue is located.

    Args:
        dataset_idx (int): The dataset index is the index of the dataset in the dataset list.
        path_to_csv (list): The path to the csv file that contains the bounding box sizes for the residues.

    Returns:
        array: The bounding box as a numpy array.
    """
    # Get the bounding box by using the cahched line access
    line = linecache.getline(path_to_csv, dataset_idx + 1)
    # Return the bounding box as a numpy array
    return np.array([float(x) for x in line.split(",")])


def get_neighbour_residues(dataset_idx: int, path_to_csv: str):
    """
    This function returns a list of the neighbour residues of a residue in a dataset.

    Args:
        dataset_idx (int): The dataset index is the index of the dataset in the dataset list.
        path_to_csv (list): The path to the csv file that contains the neighbour residues for the residues.

    Returns:
        array: List of neighbour residue indices.
    """
    # Get the neighbours
    line = linecache.getline(path_to_csv, dataset_idx + 1).replace("\n", "")
    if len(line) == 0:
        return np.array([])
    # Return the the neighbors as numpy array
    return np.array([int(x) for x in line.split(",")])


def get_size(obj, seen=None):
    """
    Recursively finds RAM size of objects
    """
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()

    obj_id = id(obj)
    if obj_id in seen:
        return 0

    # Important mark as seen *before* entering recursion to gracefully handle self-referential objects
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, "__dict__"):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, "__iter__") and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size


def scale_output_ic(ic_index: int, value: float) -> float:
    """
    Scales the output internal coordinate value based on the internal coordinate index.
    """
    ic = get_ic_from_index(ic_index)
    ic_type = get_ic_type(ic)

    if ic_type == "bond":
        # Bonds are between [1A, 1.7A] -> [0,1] -> [-0.25, 0.25]
        return (value - 1) / (2 * (1.7 - 1)) - 0.25
    elif ic_type == "angle":
        # Angles are between [90°, 160°] -> [0,1]
        return (value - np.deg2rad(90)) / (np.deg2rad(160) - np.deg2rad(90))
    elif ic_type == "dihedral":
        # Dihedrals are between [50°, 140°] -> [0,1]
        return (value - np.deg2rad(50)) / (np.deg2rad(140) - np.deg2rad(50))
    else:
        raise Exception(f"Internal coordinate type {ic_type} not supported!")


def inverse_scale_output_ic(ic_index: int, value: float) -> float:
    """
    Inversely scales the output internal coordinate value based on the internal coordinate index.
    """
    ic = get_ic_from_index(ic_index)
    ic_type = get_ic_type(ic)

    if ic_type == "bond":
        # Bonds are between [-0.25, 0.25] -> [0,1] -> [1A, 1.7A]
        return (value + 0.25) * 2 * (1.7 - 1) + 1
    elif ic_type == "angle":
        # Angles are between [0,1] -> [90°, 160°]
        return value * (np.deg2rad(160) - np.deg2rad(90)) + np.deg2rad(90)
    elif ic_type == "dihedral":
        # Dihedrals are between [0,1] ->  [50°, 140°]
        return value * (np.deg2rad(140) - np.deg2rad(50)) + np.deg2rad(50)
    else:
        raise Exception(f"Internal coordinate type {ic_type} not supported!")


class BaseDataGenerator(tf.keras.utils.Sequence):
    """
    This is the base class for the backmapping data generator.
    It is used to generate batches of data for the CNN. The children classes specify the structure of the data.
    For example, the RelativeVectorsTrainingDataGenerator generates batches of relative bond vectors.
    """

    def __init__(
        self,
        input_dir_path: str,
        output_dir_path: str,
        input_size: tuple = (11 + 2 * PADDING, 3 + 2 * PADDING, 1),
        output_size: tuple = (53 + 2 * PADDING, 3 + 2 * PADDING, 1),
        shuffle: bool = False,
        batch_size: int = 1,
        validate_split: float = 0.1,
        validation_mode: bool = False,
        augmentation: bool = False,
        data_usage: float = 1.0,
        cache_name: str = "",
        use_cache: bool = True,
    ):
        """
        This is the base class for the backmapping data generator.

        Args:
            input_dir_path (str): The path to the directory where the input data (X) is located
            output_dir_path (str): The path to the directory where the output data (Y) is located
            input_size (tuple, optional): The size/shape of the input data. Defaults to (11 + 2 * PADDING, 3 + 2 * PADDING, 1).
            output_size (tuple, optional): The size/shape of the output data. Defaults to (53 + 2 * PADDING, 3 + 2 * PADDING, 1).
            shuffle (bool, optional): If the data should be shuffled after each epoch. Defaults to False.
            batch_size (int, optional): The size of each batch. Defaults to 1.
            validate_split (float, optional): The percentage of data that should be used for validation. Defaults to 0.1.
            validation_mode (bool, optional): If the generator should be in validation mode. Defaults to False.
            augmentation (bool, optional): If the generator should augment the data. Defaults to False.
            data_usage (float, optional): The percentage of data that should be used. Defaults to 100%
        """

        # TODO: make this more foolproof and add more checks

        self.input_dir_path = input_dir_path
        self.output_dir_path = output_dir_path
        self.input_size = input_size
        self.output_size = output_size
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.validation_mode = validation_mode
        self.augmentation = augmentation
        self.use_cache = use_cache

        self.parser = PDBParser(QUIET=True)
        self.cache = {}

        max_index = int(os.listdir(input_dir_path).__len__() - 1)

        self.len = int(max_index * (1 - validate_split))
        self.start_index = 0
        self.end_index = self.len

        # Set validation mode
        if self.validation_mode:
            self.start_index = self.len + 1
            self.end_index = max_index
            self.len = self.end_index - self.start_index + 1

        # Set data use
        if data_usage < 1.0:
            self.len = np.max([int(self.len * data_usage), self.batch_size])
            self.end_index = self.start_index + self.len - 1

        # Use biggest multiple of batch size that fits
        self.len = np.max([(self.len // self.batch_size) * self.batch_size, self.batch_size])
        self.end_index = self.start_index + self.len - 1

        # Debug
        print(f"Found {self.len} residues in ({self.input_dir_path})")

        self.cache_name = cache_name

        # Make a default cache name if not set by the inheriting class
        if not self.cache_name:
            self.cache_name = ("validation" if self.validation_mode else "training") + "_" + str(self.batch_size) + "_cache"

        self.cache_path = os.path.join(config(Keys.DATA_PATH), "cache", self.cache_name + ".pkl")

        # Try load cache from file
        if os.path.exists(self.cache_path) and self.use_cache:
            try:
                with open(self.cache_path, "rb") as f:
                    self.cache = pickle.load(f)
                    print(f"Loaded cache from file ({self.cache_path}) with {len(self.cache)} batches ({len(self.cache)*self.batch_size} residues)")
            except Exception as e:
                print(f"Could not load cache from file!")
        else:
            print(f"Not using cache...")
        # Initialize
        self.on_epoch_end()

    def on_epoch_end(self):
        # TODO: save cache to file here instead of in getitem
        pass

    def __len__(self):
        return self.len // self.batch_size

    def __getitem__(self, idx):
        # Input, Output
        X, Y = None, None

        # Check if the item is cached
        cached_item = self.__load_cached_item__(idx)
        if cached_item is not None:
            X, Y = cached_item
        else:
            # Get the item
            X, Y = self.__gen_item__(idx)

            # Try to cache the item
            self.__cache_item__(idx, (X, Y))

        # Augment data
        if self.augmentation:
            X, Y = self.__augment_data__(X, Y)

        return X, Y

    def __gen_item__(self, idx):
        raise NotImplementedError("This method needs to be implemented by the inheriting class!")

    def __augment_data__(self, X, Y):
        raise NotImplementedError("This method needs to be implemented by the inheriting class!")

    def __cache_item__(self, idx, output):
        """
        This function tries to cache the output of the generator. If the cache already exists, it will be overwritten.
        Also checks if enough memories is available.

        Args:
            idx (int): The index of the batch
            output (tuple): The output (X, Y)
        """

        try:
            self.cache[idx] = output
        except MemoryError:
            print(f"Not enough memory to cache batch {idx}/{self.__len__()}!")
        except Exception as e:
            print(f"Could not cache batch {idx}/{self.__len__()}!")
            print(e)

        # Try save to file
        try:
            # Check if path exists
            if not os.path.exists(os.path.dirname(self.cache_path)):
                os.makedirs(os.path.dirname(self.cache_path))

            # Save cache to file
            with open(self.cache_path, "wb") as f:
                pickle.dump(self.cache, f, protocol=pickle.HIGHEST_PROTOCOL)

        except Exception as e:
            print(f"Could not save cache to file!")
            print(e)

    def __load_cached_item__(self, idx):
        """
        This function tries to load a cached item from the cache.

        Args:
            idx (int): The index of the batch

        Returns:
            tuple: The output (X, Y)
        """

        try:
            return self.cache[idx]
        except KeyError:
            return None
        except Exception as e:
            print(f"Could not load cached batch {idx}/{self.__len__()}!")
            print(e)
            return None

    def __get_cache_size__(self):
        return get_size(self.cache)
        # return getsizeof(self.cache)


class FICDataGenerator(BaseDataGenerator):
    """
    A data generator class that generates batches of data for the CNN.
    The input is the coarse grained structure and the output is one single free internal coordinate.
    """

    def __init__(
        self,
        input_dir_path,
        output_dir_path,
        input_size,  # We have one more bead position than relativ vectors
        output_size,  # We have one more atom position than relativ vectors
        ic_index,
        shuffle=False,
        batch_size=1,
        validate_split=0.1,
        validation_mode=False,
        augmentation=False,
        neighbourhood_size=4,
        data_usage: float = 1.0,
        use_cache: bool = True,
    ):
        """
        Args:
            input_dir_path (str): The path to the directory where the input data (X) is located
            output_dir_path (str): The path to the directory where the output data (Y) is located
            input_size (tuple, optional): The size/shape of the input data. Defaults to (11 + 2 * PADDING, 3 + 2 * PADDING, 1).
            output_size (tuple, optional): The size/shape of the output data. Defaults to (53 + 2 * PADDING, 3 + 2 * PADDING, 1).
            shuffle (bool, optional): If the data should be shuffled after each epoch. Defaults to False.
            batch_size (int, optional): The size of each batch. Defaults to 1.
            validate_split (float, optional): The percentage of data that should be used for validation. Defaults to 0.1.
            validation_mode (bool, optional): If the generator should be in validation mode. Defaults to False.
            augmentation (bool, optional): If the generator should augment the data. Defaults to False.
            ic_index (str, optional): The index of the internal coordinate that should be fitted.
            neighbourhood_size (int, optional): The amount of the neighbours to take into place. Defaults to 4.
            data_use (float, optional): The percentage of data that should be used. Defaults to 100%
            use_cache (bool, optional): If the cache should be used. Defaults to True.
        """
        # Call super constructor
        super().__init__(
            input_dir_path,
            output_dir_path,
            input_size,
            output_size,
            shuffle,
            batch_size,
            validate_split,
            validation_mode,
            augmentation,
            data_usage,
            cache_name="ICG_cache_"
            + ("validation" if validation_mode else "training")
            + "_"
            + str(ic_index)
            + "_"
            + ic_to_hlabel(get_ic_from_index(ic_index))
            + "_"
            + str(batch_size)
            + "",
            use_cache=use_cache,
        )

        self.ic_index = ic_index
        self.neighbourhood_size = neighbourhood_size

    def __gen_item__(self, idx):
        # Initialize Batch
        X = np.zeros((self.batch_size, *self.input_size), dtype=np.float32)  # Now is (batch, beades + neighborhood_size, (x, y, z), 1)
        Y = np.zeros((self.batch_size, *self.output_size), dtype=np.float32)

        # Loop over the batch
        for i in range(self.batch_size):

            # Get the index of the residue
            residue_idx = idx * self.batch_size + i

            # Check if end index is reached
            if self.validation_mode and residue_idx > self.end_index:
                raise Exception(f"You are trying to access a residue that does not exist ({residue_idx})!")

            # Get the path to the files
            cg_path = f"{self.input_dir_path}/{residue_idx}.pdb"
            at_path = f"{self.output_dir_path}/{residue_idx}.pdb"

            # Load the files
            cg_structure = self.parser.get_structure(residue_idx, cg_path)
            at_structure = self.parser.get_structure(residue_idx, at_path)

            # Get bounding box sizes
            cg_box_size = get_bounding_box(residue_idx, CG_BOUNDING_BOX_RELATION_PATH)
            at_box_size = get_bounding_box(residue_idx, AT_BOUNDING_BOX_RELATION_PATH)

            # Check if the box sizes are not nan
            if np.isnan(cg_box_size).any() or np.isnan(at_box_size).any():
                raise Exception(f"Found nan in box sizes ({cg_box_size}, {at_box_size}) in residue {residue_idx}!")

            # Get the coarse grained input (only the one molecule)
            cg_residue = list(cg_structure.get_residues())[0]  # Get the residues
            cg_atoms = list(cg_residue.get_atoms())  # Get the atoms
            neighbourhood = get_neighbour_residues(residue_idx, NEIGHBOURHOOD_PATH)  # Get the neighbourhood

            # Make the absolute positions out of a vector mapping
            X = add_absolute_positions(cg_atoms, X, i, cg_box_size)

            # Add the neighbourhood to the input
            for j in range(np.min([self.neighbourhood_size, neighbourhood.__len__()])):
                neighbor_X = self.get_neighbor_X(
                    residue_idx=neighbourhood[j], box_size=cg_box_size, position_origin=[atom.get_vector() for atom in cg_atoms if atom.get_name() == "NC3"][0]
                )[0, PADDING:-PADDING, PADDING : PADDING + 3, 0]

                # Add the neighbour to the input
                X[i, PADDING:-PADDING, PADDING + 3 * (1 + j) : PADDING + 3 * (2 + j), 0] = neighbor_X

            # Get the internal coordinate
            output_ic_value = self.__get_ic_from_cartesians(at_structure, self.ic_index, at_box_size)
            scaled_output_ic_value = scale_output_ic(self.ic_index, output_ic_value)

            # Write the internal coordinate to the output matrix
            Y[i, PADDING, PADDING, 0] = scaled_output_ic_value

        # Convert to tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        Y = tf.convert_to_tensor(Y, dtype=tf.float32)

        # Check if values that are not in [-1, 1] or [0, 1] exist
        if not is_output_matrix_healthy(Y, [0, 1]) or not is_output_matrix_healthy(X):
            if not is_output_matrix_healthy(Y, [0, 1]):
                # Find batch that is not healthy
                for i in range(Y.shape[0]):
                    if not is_output_matrix_healthy(Y[i : i + 1, :, :, :], [0, 1]):
                        print(f"Unhealty batch: {i}")
                        print_matrix(Y[i : i + 1, :, :, :])
                        break
            if not is_output_matrix_healthy(X):
                # Find batch that is not healthy
                for i in range(X.shape[0]):
                    if not is_output_matrix_healthy(X[i : i + 1, :, :, :]):
                        print(f"Unhealty batch: {i}")
                        print_matrix(X[i : i + 1, :, :, :])
                        break
            raise Exception(f"Found values outside of [-1, 1], see print before.")

        # Return tensor as deep copy
        return tf.identity(X), tf.identity(Y)

    def __get_ic_from_cartesians(self, structure: Structure, ic_index: int, box_size: np.array) -> float:
        """
        Returns the nurmerical value of an internal coordinate based on the cartesian coordinates of a structure.

        Args:
            structure (Structure): The structure of the molecule.
            ic_index (int): The index of the internal coordinate.
            box_size (np.array): The size of the simulation box.

        Returns:
            float: The numerical value of the internal coordinate. Bond lengths are in Angstrom, angles in degrees and dihedrals in radians.
        """

        extended_topology = load_extended_topology_info()  # Load the extended topology info
        ic = get_ic_from_index(self.ic_index, extended_topology)  # Get the internal coordinate info
        ic_type = get_ic_type(ic, extended_topology)  # Get the internal coordinate type (bond, angle, dihedral)

        # Calcualte the ic based on its type
        if ic_type == "bond":
            atom_i = list(structure.get_atoms())[int(ic["i"])]
            atom_j = list(structure.get_atoms())[int(ic["j"])]

            # Get the absolute positions of the atoms
            pos_i = atom_i.get_vector()
            pos_j = atom_j.get_vector()

            # Calculate the relative vector
            bond_vector = pos_j - pos_i

            # Fix the PBC
            if bond_vector.norm() > PBC_CUTOFF:
                bond_vector = fix_pbc(bond_vector, box_size)

            # Consitency check
            if bond_vector.norm() > PBC_CUTOFF:
                raise Exception(f"Found a vector that is too large ({bond_vector})!")

            return bond_vector.norm()

        elif ic_type == "angle":
            atom_i = list(structure.get_atoms())[int(ic["i"])]
            atom_j = list(structure.get_atoms())[int(ic["j"])]
            atom_k = list(structure.get_atoms())[int(ic["k"])]

            # Get the absolute positions of the atoms
            pos_i = atom_i.get_vector()
            pos_j = atom_j.get_vector()
            pos_k = atom_k.get_vector()

            # Calculate the relative vectors
            rel_vector_ji = pos_i - pos_j
            rel_vector_jk = pos_k - pos_j

            # To numpy
            rel_vector_ji = np.array([rel_vector_ji[0], rel_vector_ji[1], rel_vector_ji[2]])
            rel_vector_jk = np.array([rel_vector_jk[0], rel_vector_jk[1], rel_vector_jk[2]])

            # Fix the PBC
            if np.linalg.norm(rel_vector_ji) > PBC_CUTOFF:
                rel_vector_ji = fix_pbc(rel_vector_ji, box_size)

            if np.linalg.norm(rel_vector_jk) > PBC_CUTOFF:
                rel_vector_jk = fix_pbc(rel_vector_jk, box_size)

            # Consitency check
            if np.linalg.norm(rel_vector_ji) > PBC_CUTOFF or np.linalg.norm(rel_vector_jk) > PBC_CUTOFF:
                raise Exception(f"Found a vector that is too large ({rel_vector_ji}, {rel_vector_jk})!")

            # Calculate the angle
            angle = np.arccos(np.dot(rel_vector_ji, rel_vector_jk) / (np.linalg.norm(rel_vector_ji) * np.linalg.norm(rel_vector_jk)))

            return angle

        elif ic_type == "dihedral":
            atom_i = list(structure.get_atoms())[int(ic["i"])]
            atom_j = list(structure.get_atoms())[int(ic["j"])]
            atom_k = list(structure.get_atoms())[int(ic["k"])]
            atom_l = list(structure.get_atoms())[int(ic["l"])]

            vec1 = atom_i.get_vector() - atom_j.get_vector()
            vec2 = atom_k.get_vector() - atom_j.get_vector()
            vec3 = atom_l.get_vector() - atom_k.get_vector()

            # Make sure the vectors are numpy arrays
            vec1 = np.array([vec1[0], vec1[1], vec1[2]])
            vec2 = np.array([vec2[0], vec2[1], vec2[2]])
            vec3 = np.array([vec3[0], vec3[1], vec3[2]])

            # Fix the PBC
            if np.linalg.norm(vec1) > PBC_CUTOFF:
                vec1 = fix_pbc(vec1, box_size)

            if np.linalg.norm(vec2) > PBC_CUTOFF:
                vec2 = fix_pbc(vec2, box_size)

            if np.linalg.norm(vec3) > PBC_CUTOFF:
                vec3 = fix_pbc(vec3, box_size)

            # Consitency check
            if np.linalg.norm(vec1) > PBC_CUTOFF or np.linalg.norm(vec2) > PBC_CUTOFF or np.linalg.norm(vec3) > PBC_CUTOFF:
                raise Exception(f"Found a vector that is too large ({vec1}, {vec2}, {vec3})!")

            # Calculate the dihedral
            dihedral = np.arccos(
                np.dot(
                    np.cross(vec1, vec2) / np.linalg.norm(np.cross(vec1, vec2)),
                    np.cross(vec2, vec3) / np.linalg.norm(np.cross(vec2, vec3)),
                )
                / (np.linalg.norm(np.cross(vec1, vec2)) * np.linalg.norm(np.cross(vec2, vec3)))
            )

            return dihedral

        else:
            raise Exception(f"Internal coordinate type {ic_type} not supported!")

    def get_neighbor_X(self, residue_idx, box_size, position_origin):
        # Initialize Batch
        X = np.zeros((self.batch_size, *self.input_size), dtype=np.float32)  # Now is (batch, beades

        # Check if end index is reached
        if self.validation_mode and residue_idx > self.end_index:
            raise Exception(f"You are trying to access a residue that does not exist ({residue_idx})!")

        # Get the path to the files
        cg_path = f"{self.input_dir_path}/{residue_idx}.pdb"

        # Load the files
        cg_structure = self.parser.get_structure(residue_idx, cg_path)

        # Get the residues
        cg_residue = list(cg_structure.get_residues())[0]

        # Get the atoms
        cg_atoms = list(cg_residue.get_atoms())

        # Make the absolute positions out of a vector mapping
        X = add_absolute_positions(cg_atoms, X, 0, box_size, preset_position_origin=position_origin)

        # Convert to tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)

        # Return tensor as deep copy
        return tf.identity(X)

    def __augment_data__(self, X, Y) -> tuple:
        """
        This augments the input data, because an internal coordinate is not dependent on the orientation of the molecule,
        this will only rotate the input data.
        """

        # Make into numpy array
        X = X.numpy()

        # Augment the data by randomly rotating the dataset
        for i in range(self.batch_size):
            vectors_X = X[i, PADDING:-PADDING, PADDING:-PADDING, 0]

            # Randomly rotate the dataset
            max_angle = np.pi

            angle_x = random.uniform(-max_angle, max_angle)
            angle_y = random.uniform(-max_angle, max_angle)
            angle_z = random.uniform(-max_angle, max_angle)

            # Loop over beads
            for j in range(12):
                for k in range(int(vectors_X.shape[1] / 3)):
                    vec = vectors_X[j, k * 3 : k * 3 + 3]

                    # Rotate
                    vec = np.matmul(np.array([[1, 0, 0], [0, np.cos(angle_x), -np.sin(angle_x)], [0, np.sin(angle_x), np.cos(angle_x)]]), vec)
                    vec = np.matmul(np.array([[np.cos(angle_y), 0, np.sin(angle_y)], [0, 1, 0], [-np.sin(angle_y), 0, np.cos(angle_y)]]), vec)
                    vec = np.matmul(np.array([[np.cos(angle_z), -np.sin(angle_z), 0], [np.sin(angle_z), np.cos(angle_z), 0], [0, 0, 1]]), vec)

                    # Write back
                    vectors_X[j, k * 3 : k * 3 + 3] = vec

                # Write the rotated vectors back to the matrix
                X[i, PADDING:-PADDING, PADDING:-PADDING, 0] = vectors_X

        # Make into tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)

        return X, Y
