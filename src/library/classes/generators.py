import json
import linecache
import os
import pickle
import random
import sys

import numpy as np
import tensorflow as tf
from Bio.PDB.PDBParser import PDBParser

from library.config import Keys, config
from library.datagen.topology import get_ic_from_index, get_ic_type, ic_to_hlabel, load_extended_topology_info
from library.static.vector_mappings import DOPC_AT_MAPPING, DOPC_CG_MAPPING

PADDING_X = 1  # Padding on left and right
PADDING_Y = 1  # Padding on top and bottom

PBC_CUTOFF = 10.0  # If a bond is longer than this, it is considered to be on the other side of the simulation box
BOX_SCALE_FACTOR = 10.0  # The scale factor to downscale the bond vectors with
CG_BOUNDING_BOX_RELATION_PATH = os.path.join(config(Keys.DATA_PATH), "box_sizes_cg.csv")  # Path to the csv file that contains the bounding box sizes for the cg residues
AT_BOUNDING_BOX_RELATION_PATH = os.path.join(config(Keys.DATA_PATH), "box_sizes_at.csv")  # This is generated by the gen_train_data.py script
NEIGHBOURHOOD_PATH = os.path.join(config(Keys.DATA_PATH), "neighborhoods.csv")  # This is generated by the gen_train_data.py script
ABSOLUTE_POSITION_SCALE = 200.0  # This factor is used to scale down bead positions


def fix_pbc(vector, box_size, cutoff=[PBC_CUTOFF, PBC_CUTOFF, PBC_CUTOFF]):
    """
    This function fixes the periodic boundary conditions of a vector. It does this by checking if the vector is outside of the box and if it is, it will move it back into the box.

    Args:
        vector (vector): The vector that should be fixed.
        box_size (vector, optional): The box size of the simulation.
        cutoff (list, optional): Cutoff radius to apply the PBC fix to. Defaults to [PBC_CUTOFF, PBC_CUTOFF, PBC_CUTOFF].

    Returns:
        vector: The fixed vector.
    """
    if vector[0] > cutoff[0]:
        vector[0] -= box_size[0]
    elif vector[0] < -cutoff[0]:
        vector[0] += box_size[0]
    if vector[1] > cutoff[1]:
        vector[1] -= box_size[1]
    elif vector[1] < -cutoff[1]:
        vector[1] += box_size[1]
    if vector[2] > cutoff[2]:
        vector[2] -= box_size[2]
    elif vector[2] < -cutoff[2]:
        vector[2] += box_size[2]

    if vector[0] > cutoff[0] or vector[0] < -cutoff[0] or vector[1] > cutoff[1] or vector[1] < -cutoff[1] or vector[2] > cutoff[2] or vector[2] < -cutoff[2]:
        return fix_pbc(vector, box_size)
    else:
        return vector


def is_output_matrix_healthy(output, range=[-1, 1]):
    """
    This is a diagnostic function that checks if the output matrix is fulffiling the requirements.
    Checked will be:
        - If values outside of [-1, 1] exist (this should not be the case)
        - If nan or inf exist (this should not be the case)
    TODO: This functions can be extended to check for other things as well.
    """
    healthy = True

    # Check if values that are not in [-1, 1] exist
    if np.max(output) > range[1] or np.min(output) < range[0]:
        healthy = False

    # Check for nan or inf
    if np.isnan(output).any() or np.isinf(output).any():
        healthy = False

    return healthy


def add_relative_vectors(atom_pos_dict, atom_mapping, output_matrix, batch_index, box):
    """
    This functions calculates the relative vectors between the atoms in the atom mapping and writes them to the output matrix.
    Additionally, it fixes PBC and checks for consistency.

    Args:
        atom_pos_dict (_type_): The atom pos dict is a dict that maps the atom name to the atom position.
        atom_mapping (_type_): The atom mapping is a list of tuples that contain the atom names that should be mapped.
        output_matrix (_type_): The output matrix is the matrix where the relative vectors should be written to. Shape: (batch_size, i, j, 1)
        batch_index (_type_): The batch index is the index of the batch in the output matrix.
        residue_idx (_type_): The residue index is the index of the residue in the dataset.
    """
    for j, (at1_name, at2_name) in enumerate(atom_mapping):
        if not (atom_pos_dict.get(at1_name) and atom_pos_dict.get(at2_name)):
            raise Exception(f"Missing {at1_name} or {at2_name} in residue")
        else:
            # Calculate the relative vector
            rel_vector = atom_pos_dict.get(at2_name).get_vector() - atom_pos_dict.get(at1_name).get_vector()

            # Fix the PBC
            if rel_vector.norm() > PBC_CUTOFF:
                rel_vector = fix_pbc(rel_vector, box)

            # Consitency check
            if rel_vector.norm() > PBC_CUTOFF:
                raise Exception(f"Found a vector that is too large ({rel_vector})!")

            # Check if nan or inf is in the vector
            if (
                np.isnan(rel_vector[0])
                or np.isnan(rel_vector[1])
                or np.isnan(rel_vector[2])
                or np.isinf(rel_vector[0])
                or np.isinf(rel_vector[1])
                or np.isinf(rel_vector[2])
            ):
                raise Exception(f"Found nan or inf in vector ({rel_vector})!")

        # Write the relative vectors to the output matrix
        for k in range(3):
            output_matrix[batch_index, j + PADDING_X, PADDING_Y + k, 0] = rel_vector[k] / BOX_SCALE_FACTOR

    return output_matrix


def add_absolute_positions(atoms, output_matrix, batch_index, box, target_atom=None, preset_position_origin=None, position_scale=ABSOLUTE_POSITION_SCALE):
    """
    This functions calculates the absolute positions of the atoms in the atom mapping and writes them to the output matrix.

    Args:
        atom_pos_dict (_type_): The atom pos dict is a dict that maps the atom name to the atom position.
        atom_mapping (_type_): The atom mapping is a list of tuples that contain the atom names that should be mapped.
        output_matrix (_type_): The output matrix is the matrix where the relative vectors should be written to. Shape: (batch_size, i, j, 1)
        batch_index (_type_): The batch index is the index of the batch in the output matrix.
        residue_idx (_type_): The residue index is the index of the residue in the dataset.
        target_name (_type_): The target is the atom that should be fitted. If this is None, all atoms will be fitted. If a target is set, only this atom will be in the output matrix.
        preset_position_origin (_type_): The position origin is the position of the atom that should be fitted. If this is None, the position of the N atom or NC3 bead will be used.
    """
    # Filter out hydrogen atoms
    atoms = [atom for atom in atoms if atom.element != "H"]

    # Make base position
    position_origin = [atom for atom in atoms if atom.get_name() == "N" or atom.get_name() == "NC3"][0].get_vector()  # Everything is relative to the N atom or NC3 bead
    if preset_position_origin:
        position_origin = preset_position_origin

    # Remove the N atom or NC3 bead because their position is the base position (0,0,0)
    if not preset_position_origin:
        atoms = [atom for atom in atoms if atom.get_name() != "N" and atom.get_name() != "NC3"]

    if target_atom:
        # If only one atom is selected we need to filter out all other atoms
        atoms = [atom for atom in atoms if atom.get_name() == target_atom.get_name()]

        if len(atoms) != 1:
            raise Exception(f"Found {len(atoms)} atoms with the name {target_atom.get_name()}!")

    for j, atom in enumerate(atoms):
        # Calculate the relative vector
        position = atom.get_vector() - position_origin

        # Fix the PBC
        position = fix_pbc(position, box, cutoff=[box[0] / 1.5, box[1] / 1.5, box[2] / 1.5])  # TODO: Maybe find better cutoff approximations

        # Check if nan or inf is in the vector
        if np.isnan(position[0]) or np.isnan(position[1]) or np.isnan(position[2]) or np.isinf(position[0]) or np.isinf(position[1]) or np.isinf(position[2]):
            raise Exception(f"Found nan or inf in vector ({position})!")

        # TODO: translate to [0,1] here

        # Write the relative vectors to the output matrix
        for k in range(3):
            output_matrix[batch_index, j + PADDING_X, PADDING_Y + k, 0] = (
                position[k] / position_scale
            )  # <- Position scale factor depends on if the atom is a bead or an atom (beads contain possible neighbors and cover a much larger area!)

    return output_matrix


def get_bounding_box(dataset_idx: int, path_to_csv: str) -> np.array:
    """
    This function returns the bounding box of a residue in a dataset. The bounding box is the size of the simulation box in which the residue is located.

    Args:
        dataset_idx (int): The dataset index is the index of the dataset in the dataset list.
        path_to_csv (list): The path to the csv file that contains the bounding box sizes for the residues.

    Returns:
        array: The bounding box as a numpy array.
    """
    # Get the bounding box by using the cahched line access
    line = linecache.getline(path_to_csv, dataset_idx + 1)
    # Return the bounding box as a numpy array
    return np.array([float(x) for x in line.split(",")])


def get_neighbour_residues(dataset_idx: int, path_to_csv: str):
    """
    This function returns a list of the neighbour residues of a residue in a dataset.

    Args:
        dataset_idx (int): The dataset index is the index of the dataset in the dataset list.
        path_to_csv (list): The path to the csv file that contains the neighbour residues for the residues.

    Returns:
        array: List of neighbour residue indices.
    """
    # Get the bounding box by using the cahched line access
    line = linecache.getline(path_to_csv, dataset_idx + 1).replace("\n", "")
    if len(line) == 0:
        return np.array([])
    # Return the bounding box as a numpy array
    return np.array([int(x) for x in line.split(",")])


def print_matrix(matrix):
    """
    This function prints a matrix in ascii art.

    Args:
        matrix (list): List with shape (batch_size, i, j, 1)
    """
    # Prints an output/input matrix in ascii art
    for i in range(matrix.shape[0]):
        print(" ", end="")
        for k in range(matrix.shape[1]):
            print(f"{k:6d}", end="  ")
        print()
        print(matrix.shape[1] * 8 * "-")
        # Batch
        for j in range(matrix.shape[2]):
            # Y
            for k in range(matrix.shape[1]):
                # X
                minus_sign_padding = " " if matrix[i, k, j, 0] >= 0 else ""
                print(f"{minus_sign_padding}{matrix[i, k, j, 0]:.4f}", end=" ")
            print()
        print(matrix.shape[1] * 8 * "-")


def get_size(obj, seen=None):
    """
    Recursively finds RAM size of objects
    """
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()

    obj_id = id(obj)
    if obj_id in seen:
        return 0

    # Important mark as seen *before* entering recursion to gracefully handle self-referential objects
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, "__dict__"):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, "__iter__") and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size


def get_mean_distance_and_std(ic_index: str) -> tuple:
    """
    Returns the mean distance and standard deviation for a given atom name.

    Args:
        ic_index (str): The name of the atom to get the mean distance and standard deviation for.

    Returns:
        tuple: A tuple containing the mean distance and standard deviation for the given atom name.
    """

    # Get mean distances to color the plot accordingly
    mean_distances = {}
    std = {}

    # Read csv
    with open(os.path.join(config(Keys.DATA_PATH), "mean_distances.csv"), "r") as f:
        # Skip header
        f.readline()

        for line in f.readlines():
            # Split line
            line = line.split(",")
            # Get mean distance
            mean_distances[line[0]] = float(line[1])
            # Get std
            std[line[0]] = float(line[2])

    return mean_distances[ic_index], std[ic_index]


def get_scale_factor(ic_index):
    mean, std = get_mean_distance_and_std(ic_index)
    return mean + 2 * std


class BaseDataGenerator(tf.keras.utils.Sequence):
    """
    This is the base class for the backmapping data generator.
    It is used to generate batches of data for the CNN. The children classes specify the structure of the data.
    For example, the RelativeVectorsTrainingDataGenerator generates batches of relative bond vectors.
    """

    def __init__(
        self,
        input_dir_path: str,
        output_dir_path: str,
        input_size: tuple = (11 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1),
        output_size: tuple = (53 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1),
        shuffle: bool = False,
        batch_size: int = 1,
        validate_split: float = 0.1,
        validation_mode: bool = False,
        augmentation: bool = False,
        data_usage: float = 1.0,
        cache_name: str = "",
    ):
        """
        This is the base class for the backmapping data generator.

        Args:
            input_dir_path (str): The path to the directory where the input data (X) is located
            output_dir_path (str): The path to the directory where the output data (Y) is located
            input_size (tuple, optional): The size/shape of the input data. Defaults to (11 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1).
            output_size (tuple, optional): The size/shape of the output data. Defaults to (53 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1).
            shuffle (bool, optional): If the data should be shuffled after each epoch. Defaults to False.
            batch_size (int, optional): The size of each batch. Defaults to 1.
            validate_split (float, optional): The percentage of data that should be used for validation. Defaults to 0.1.
            validation_mode (bool, optional): If the generator should be in validation mode. Defaults to False.
            augmentation (bool, optional): If the generator should augment the data. Defaults to False.
            data_usage (float, optional): The percentage of data that should be used. Defaults to 100%
        """
        self.input_dir_path = input_dir_path
        self.output_dir_path = output_dir_path
        self.input_size = input_size
        self.output_size = output_size
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.validation_mode = validation_mode
        self.augmentation = augmentation

        self.parser = PDBParser(QUIET=True)
        self.cache = {}

        max_index = int(os.listdir(input_dir_path).__len__() - 1)

        self.len = int(max_index * (1 - validate_split))
        self.start_index = 0
        self.end_index = self.len

        # Set validation mode
        if self.validation_mode:
            self.start_index = self.len + 1
            self.end_index = max_index
            self.len = self.end_index - self.start_index + 1

        # Set data use
        if data_usage < 1.0:
            self.len = np.max([int(self.len * data_usage), self.batch_size])
            self.end_index = self.start_index + self.len - 1

        # Use biggest multiple of batch size that fits
        self.len = np.max([(self.len // self.batch_size) * self.batch_size, self.batch_size])
        self.end_index = self.start_index + self.len - 1

        # Debug
        print(f"Found {self.len} residues in ({self.input_dir_path})")

        self.cache_name = cache_name

        # Make a default cache name if not set by the inheriting class
        if not self.cache_name:
            self.cache_name = ("validation" if self.validation_mode else "training") + "_" + str(self.batch_size) + "_cache"

        self.cache_path = os.path.join(config(Keys.DATA_PATH), self.cache_name + ".pkl")

        # Try load cache from file
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, "rb") as f:
                    self.cache = pickle.load(f)
                    print(f"Loaded cache from file ({self.cache_path}) with {len(self.cache)} items!")
            except Exception as e:
                print(f"Could not load cache from file!")

        # Initialize
        self.on_epoch_end()

    def on_epoch_end(self):
        pass

    def __len__(self):
        return self.len // self.batch_size

    def __getitem__(self, idx):
        # Input, Output
        X, Y = None, None

        # Check if the item is cached
        cached_item = self.__load_cached_item__(idx)
        if cached_item is not None:
            X, Y = cached_item
        else:
            # Get the item
            X, Y = self.__gen_item__(idx)

            # Try to cache the item
            self.__cache_item__(idx, (X, Y))

        # Augment data
        if self.augmentation:
            X, Y = self.__augment_data__(X, Y)

        return X, Y

    def __gen_item__(self, idx):
        raise NotImplementedError("This method needs to be implemented by the inheriting class!")

    def __augment_data__(self, X, Y):
        raise NotImplementedError("This method needs to be implemented by the inheriting class!")

    def __cache_item__(self, idx, output):
        """
        This function tries to cache the output of the generator. If the cache already exists, it will be overwritten.
        Also checks if enough memories is available.

        Args:
            idx (int): The index of the batch
            output (tuple): The output (X, Y)
        """

        try:
            self.cache[idx] = output
        except MemoryError:
            print(f"Not enough memory to cache batch {idx}/{self.__len__()}!")
        except Exception as e:
            print(f"Could not cache batch {idx}/{self.__len__()}!")
            print(e)

        # Try save to file
        try:
            with open(self.cache_path, "wb") as f:
                pickle.dump(self.cache, f, protocol=pickle.HIGHEST_PROTOCOL)
        except Exception as e:
            print(f"Could not save cache to file!")
            print(e)

    def __load_cached_item__(self, idx):
        """
        This function tries to load a cached item from the cache.

        Args:
            idx (int): The index of the batch

        Returns:
            tuple: The output (X, Y)
        """

        try:
            return self.cache[idx]
        except KeyError:
            return None
        except Exception as e:
            print(f"Could not load cached batch {idx}/{self.__len__()}!")
            print(e)
            return None

    def __get_cache_size__(self):
        return get_size(self.cache)
        # return getsizeof(self.cache)


class FICDataGenerator(BaseDataGenerator):
    """
    A data generator class that generates batches of data for the CNN.
    The input is the coarse grained structure and the output is one single free internal coordinate.
    """

    def __init__(
        self,
        input_dir_path,
        output_dir_path,
        input_size,  # We have one more bead position than relativ vectors
        output_size,  # We have one more atom position than relativ vectors
        ic_index,
        shuffle=False,
        batch_size=1,
        validate_split=0.1,
        validation_mode=False,
        augmentation=False,
        neighbourhood_size=4,
        data_usage: float = 1.0,
    ):
        """
        Args:
            input_dir_path (str): The path to the directory where the input data (X) is located
            output_dir_path (str): The path to the directory where the output data (Y) is located
            input_size (tuple, optional): The size/shape of the input data. Defaults to (11 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1).
            output_size (tuple, optional): The size/shape of the output data. Defaults to (53 + 2 * PADDING_X, 3 + 2 * PADDING_Y, 1).
            shuffle (bool, optional): If the data should be shuffled after each epoch. Defaults to False.
            batch_size (int, optional): The size of each batch. Defaults to 1.
            validate_split (float, optional): The percentage of data that should be used for validation. Defaults to 0.1.
            validation_mode (bool, optional): If the generator should be in validation mode. Defaults to False.
            augmentation (bool, optional): If the generator should augment the data. Defaults to False.
            ic_index (str, optional): The index of the internal coordinate that should be fitted.
            neighbourhood_size (int, optional): The amount of the neighbours to take into place. Defaults to 4.
            data_use (float, optional): The percentage of data that should be used. Defaults to 100%
        """
        # Call super constructor
        super().__init__(
            input_dir_path,
            output_dir_path,
            input_size,
            output_size,
            shuffle,
            batch_size,
            validate_split,
            validation_mode,
            augmentation,
            data_usage,
            cache_name="ICG_cache_"
            + ("validation" if validation_mode else "training")
            + "_"
            + str(ic_index)
            + "_"
            + ic_to_hlabel(get_ic_from_index(ic_index))
            + "_"
            + str(batch_size)
            + "",
        )

        self.ic_index = ic_index
        self.neighbourhood_size = neighbourhood_size

    def __gen_item__(self, idx):
        # Initialize Batch
        X = np.zeros((self.batch_size, *self.input_size), dtype=np.float32)  # Now is (batch, beades + neighborhood_size, (x, y, z), 1)
        Y = np.zeros((self.batch_size, *self.output_size), dtype=np.float32)

        # Loop over the batch
        for i in range(self.batch_size):
            # Get the index of the residue
            residue_idx = idx * self.batch_size + i

            # Check if end index is reached
            if self.validation_mode and residue_idx > self.end_index:
                raise Exception(f"You are trying to access a residue that does not exist ({residue_idx})!")

            # Get the path to the files
            cg_path = f"{self.input_dir_path}/{residue_idx}.pdb"
            at_path = f"{self.output_dir_path}/{residue_idx}.pdb"

            # Load the files
            cg_structure = self.parser.get_structure(residue_idx, cg_path)
            at_structure = self.parser.get_structure(residue_idx, at_path)

            # Get bounding box sizes
            cg_box_size = get_bounding_box(residue_idx, CG_BOUNDING_BOX_RELATION_PATH)
            at_box_size = get_bounding_box(residue_idx, AT_BOUNDING_BOX_RELATION_PATH)

            # Check if the box sizes are not nan
            if np.isnan(cg_box_size).any() or np.isnan(at_box_size).any():
                raise Exception(f"Found nan in box sizes ({cg_box_size}, {at_box_size}) in residue {residue_idx}!")

            # Get the coarse grained input (only the one molecule)
            cg_residue = list(cg_structure.get_residues())[0]  # Get the residues
            cg_atoms = list(cg_residue.get_atoms())  # Get the atoms
            neighbourhood = get_neighbour_residues(residue_idx, NEIGHBOURHOOD_PATH)  # Get the neighbourhood

            # Make the absolute positions out of a vector mapping
            X = add_absolute_positions(cg_atoms, X, i, cg_box_size)

            # Add the neighbourhood to the input
            for j in range(np.min([self.neighbourhood_size, neighbourhood.__len__()])):
                neighbor_X = self.get_neighbor_X(
                    residue_idx=neighbourhood[j], box_size=cg_box_size, position_origin=[atom.get_vector() for atom in cg_atoms if atom.get_name() == "NC3"][0]
                )[0, PADDING_X : 12 + PADDING_X, PADDING_Y:-PADDING_Y, 0]

                # Add the neighbour to the input
                X[i, PADDING_X + 12 + j * 12 : PADDING_X + 24 + j * 12, PADDING_Y:-PADDING_Y, 0] = neighbor_X

            # Get the internal coordinate
            extended_topology = load_extended_topology_info()
            ic = get_ic_from_index(self.ic_index, extended_topology)
            ic_type = get_ic_type(ic, extended_topology)

            # Calcualte the ic based on its type
            if ic_type == "bond":
                atom_i = list(at_structure.get_atoms())[int(ic["i"])]
                atom_j = list(at_structure.get_atoms())[int(ic["j"])]

                # Get the absolute positions of the atoms
                pos_i = atom_i.get_vector()
                pos_j = atom_j.get_vector()

                # Calculate the relative vector
                rel_vector = pos_j - pos_i

                # Fix the PBC
                if rel_vector.norm() > PBC_CUTOFF:
                    rel_vector = fix_pbc(rel_vector, at_box_size)

                # Consitency check
                if rel_vector.norm() > PBC_CUTOFF:
                    raise Exception(f"Found a vector that is too large ({rel_vector})!")

                # Write the internal coordinate to the output matrix
                Y[i, PADDING_X, PADDING_Y, 0] = rel_vector.norm() / BOX_SCALE_FACTOR

            elif ic_type == "angle":
                atom_i = list(at_structure.get_atoms())[int(ic["i"])]
                atom_j = list(at_structure.get_atoms())[int(ic["j"])]
                atom_k = list(at_structure.get_atoms())[int(ic["k"])]

                # print(atom_i.name, atom_j.name, atom_k.name)

                # Get the absolute positions of the atoms
                pos_i = atom_i.get_vector()
                pos_j = atom_j.get_vector()
                pos_k = atom_k.get_vector()

                # Calculate the relative vectors
                rel_vector_ji = pos_i - pos_j
                rel_vector_jk = pos_k - pos_j

                # To numpy
                rel_vector_ji = np.array([rel_vector_ji[0], rel_vector_ji[1], rel_vector_ji[2]])
                rel_vector_jk = np.array([rel_vector_jk[0], rel_vector_jk[1], rel_vector_jk[2]])

                # Fix the PBC
                if np.linalg.norm(rel_vector_ji) > PBC_CUTOFF:
                    rel_vector_ji = fix_pbc(rel_vector_ji, at_box_size)

                if np.linalg.norm(rel_vector_jk) > PBC_CUTOFF:
                    rel_vector_jk = fix_pbc(rel_vector_jk, at_box_size)

                # Consitency check
                if np.linalg.norm(rel_vector_ji) > PBC_CUTOFF or np.linalg.norm(rel_vector_jk) > PBC_CUTOFF:
                    raise Exception(f"Found a vector that is too large ({rel_vector_ji}, {rel_vector_jk})!")

                # Calculate the angle
                angle = np.arccos(np.dot(rel_vector_ji, rel_vector_jk) / (np.linalg.norm(rel_vector_ji) * np.linalg.norm(rel_vector_jk)))

                # To degrees
                angle = np.degrees(angle)
                # print(angle, extended_topology.angles[self.ic_index])

                # Write the internal coordinate to the output matrix
                Y[i, PADDING_X, PADDING_Y, 0] = angle / (360 * BOX_SCALE_FACTOR)

            elif ic_type == "dihedral":
                i, j, k, l = ic["i"], ic["j"], ic["k"], ic["l"]

                # TODO: calculate the dihedral between i, j, k and l

            else:
                raise Exception(f"Internal coordinate type {ic_type} not supported!")

        # Convert to tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        Y = tf.convert_to_tensor(Y, dtype=tf.float32)

        # Check if values that are not in [-1, 1] or [0, 1] exist
        if not is_output_matrix_healthy(Y, [0, 1]) or not is_output_matrix_healthy(X):
            if not is_output_matrix_healthy(Y, [0, 1]):
                # Find batch that is not healthy
                for i in range(Y.shape[0]):
                    if not is_output_matrix_healthy(Y[i : i + 1, :, :, :], [0, 1]):
                        print(f"Unhealty batch: {i}")
                        print_matrix(Y[i : i + 1, :, :, :])
                        break
            if not is_output_matrix_healthy(X):
                # Find batch that is not healthy
                for i in range(X.shape[0]):
                    if not is_output_matrix_healthy(X[i : i + 1, :, :, :]):
                        print(f"Unhealty batch: {i}")
                        print_matrix(X[i : i + 1, :, :, :])
                        break
            raise Exception(f"Found values outside of [-1, 1], see print before.")

        # Return tensor as deep copy
        return tf.identity(X), tf.identity(Y)

    def get_neighbor_X(self, residue_idx, box_size, position_origin):
        # Initialize Batch
        X = np.zeros((self.batch_size, *self.input_size), dtype=np.float32)  # Now is (batch, beades

        # Check if end index is reached
        if self.validation_mode and residue_idx > self.end_index:
            raise Exception(f"You are trying to access a residue that does not exist ({residue_idx})!")

        # Get the path to the files
        cg_path = f"{self.input_dir_path}/{residue_idx}.pdb"

        # Load the files
        cg_structure = self.parser.get_structure(residue_idx, cg_path)

        # Get the residues
        cg_residue = list(cg_structure.get_residues())[0]

        # Get the atoms
        cg_atoms = list(cg_residue.get_atoms())

        # Make the absolute positions out of a vector mapping
        X = add_absolute_positions(cg_atoms, X, 0, box_size, preset_position_origin=position_origin)

        # Convert to tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)

        # Return tensor as deep copy
        return tf.identity(X)

    def __augment_data__(self, X, Y):
        """
        This augments the input data, because an internal coordinate is not dependent on the orientation of the molecule,
        this will only rotate the input data.
        """

        # Make into numpy array
        X = X.numpy()

        # Augment the data by randomly rotating the dataset
        for i in range(self.batch_size):
            vectors_X = X[i, :, :, 0]

            # Randomly rotate the dataset
            max_angle = 2 * np.pi

            angle_x = random.uniform(-max_angle, max_angle)
            angle_y = random.uniform(-max_angle, max_angle)
            angle_z = random.uniform(-max_angle, max_angle)

            # Loop over beads
            for j in range(vectors_X.shape[0]):
                vec = vectors_X[j, PADDING_Y:-PADDING_Y]
                # Rotate
                vec = np.matmul(np.array([[1, 0, 0], [0, np.cos(angle_x), -np.sin(angle_x)], [0, np.sin(angle_x), np.cos(angle_x)]]), vec)
                vec = np.matmul(np.array([[np.cos(angle_y), 0, np.sin(angle_y)], [0, 1, 0], [-np.sin(angle_y), 0, np.cos(angle_y)]]), vec)
                vec = np.matmul(np.array([[np.cos(angle_z), -np.sin(angle_z), 0], [np.sin(angle_z), np.cos(angle_z), 0], [0, 0, 1]]), vec)

                # Write back
                vectors_X[j, PADDING_Y:-PADDING_Y] = vec

            # Write the rotated vectors back to the matrix
            X[i, :, :, 0] = vectors_X

        # Make into tensor
        X = tf.convert_to_tensor(X, dtype=tf.float32)

        return X, Y
